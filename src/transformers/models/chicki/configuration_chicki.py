# coding=utf-8
## Licence d'Utilisation Chicki
#
#Copyright (c) 2024 Chicki. Tous droits réservés.
#
#Cette licence régit l'utilisation du modèle Chicki. En utilisant, copiant ou distribuant ce modèle, vous acceptez de vous conformer aux termes et conditions suivants.
#
### 1. Politique d'Utilisation Acceptable
#
#Vous devez respecter la Politique d'Utilisation Acceptable de Chicki comme détaillée ci-dessous. Cela inclut des restrictions sur les activités illégales, les comportements nuisibles, et l'utilisation abusive du modèle.
#
#### 1.1 Usages Interdits
#Vous vous engagez à ne pas utiliser le modèle, ni à permettre à d'autres de l'utiliser, pour toute finalité qui :
#
#1. Violerait une loi ou porterait atteinte aux droits d'autrui, y compris :
#   - Participer à ou promouvoir des activités illégales telles que la violence, le terrorisme, la traite des êtres humains, l'exploitation de mineurs, ou tout autre acte illicite.
#   - Faciliter le harcèlement, les abus, la discrimination, ou l'exercice non autorisé de toute profession.
#   - Collecter ou traiter des données personnelles sensibles sans autorisation appropriée.
#   - Créer ou distribuer des logiciels malveillants ou perturber autrement les systèmes numériques.
#
#2. Impliquerait des activités présentant un risque de préjudice physique ou de mort pour des individus, telles que :
#   - Applications liées à des activités militaires, nucléaires, ou d'espionnage.
#   - Développement ou utilisation d'armes illégales, de drogues ou de substances contrôlées.
#   - Exploitation d'infrastructures critiques, de technologies de transport ou de machines lourdes.
#   - Promouvoir l'automutilation, le suicide ou toute autre forme de préjudice corporel.
#
#3. Tromperait ou induirait intentionnellement en erreur les autres, y compris :
#   - Générer ou promouvoir du contenu frauduleux, de la désinformation, ou des déclarations diffamatoires.
#   - Distribuer du spam ou se faire passer pour autrui sans consentement.
#   - Représenter à tort que les sorties du modèle sont générées par des humains.
#   - Faciliter un engagement en ligne faux, comme de fausses critiques.
#
#4. Ne divulgue pas de manière adéquate aux utilisateurs finaux les risques connus associés à votre système d'IA.
#
### 2. Signalement des Violations
#
#Les violations de cette Politique, ainsi que les bugs logiciels ou les problèmes de sécurité, doivent être signalés via les canaux suivants :
#- Signaler des problèmes avec le modèle : [https://github.com/ChickiLM](https://github.com/ChickiLM)
#- Signaler des bugs et des problèmes de sécurité : Chicki-Bugs@llm.ci
#- Signaler des violations de la Politique d'Utilisation Acceptable : Chicki-Viola@llm.ci
#
### 3. Avertissement
#
#LE MODÈLE EST FOURNI "TEL QUEL", SANS AUCUNE GARANTIE, EXPRESSE OU IMPLICITE, Y COMPRIS MAIS SANS S'Y LIMITER AUX GARANTIES DE QUALITÉ MARCHANDE, D'ADÉQUATION À UN USAGE PARTICULIER, ET DE NON-VIOLATION. EN AUCUN CAS LE DÉTENTEUR DU DROIT D'AUTEUR OU LES CONTRIBUTEURS NE SERONT RESPONSABLES DE TOUTE RÉCLAMATION, DOMMAGE OU AUTRE RESPONSABILITÉ, QUE CE SOIT DANS LE CADRE D'UN CONTRAT, D'UN DÉLIT OU AUTREMENT, DÉCOULANT DE, DEPUIS OU EN LIEN AVEC LE MODÈLE OU L'UTILISATION OU AUTRES INTERACTIONS AVEC LE MODÈLE.
#
### 4. Droit Applicable
#
#Cette licence est régie et interprétée conformément aux lois françaises, à l'exclusion de ses dispositions relatives aux conflits de lois.
#
### 5. Résiliation
#
#Cette licence est effective jusqu'à sa résiliation. Vos droits en vertu de cette licence seront automatiquement résiliés sans préavis si vous ne respectez pas les termes. À la résiliation, vous devez cesser toute utilisation et détruire toutes les copies du modèle.
#
### 6. Informations de Contact
#
#Pour toute question ou problème lié à cette licence, veuillez contacter Chicki à Chicki-Contact@llm.ci.
#
#
#
## Chicki License Agreement
#
#Copyright (c) 2024 Chicki. All rights reserved.
#
#This license governs the use of the Chicki model. By using, copying, or distributing this model, you agree to comply with the following terms and conditions.
#
### 1. Acceptable Use Policy
#
#You must comply with the Chicki Acceptable Use Policy as detailed below. This includes restrictions on illegal activities, harmful behavior, and misuse of the model.
#
#### 1.1 Prohibited Uses
#You agree not to use the model, nor allow others to use it, for any purpose that:
#
#1. Violates any law or infringes the rights of others, including:
#   - Engaging in or promoting illegal activities such as violence, terrorism, human trafficking, exploitation of minors, or any other unlawful act.
#   - Facilitating harassment, abuse, discrimination, or unauthorized practice of any profession.
#   - Collecting or processing sensitive personal data without proper authorization.
#   - Creating or distributing malicious software or otherwise disrupting digital systems.
#
#2. Involves activities that pose a risk of physical harm or death to individuals, such as:
#   - Applications related to military, nuclear, or espionage activities.
#   - Development or use of illegal weapons, drugs, or controlled substances.
#   - Exploitation of critical infrastructure, transportation technologies, or heavy machinery.
#   - Promoting self-harm, suicide, or other forms of bodily harm.
#
#3. Intentionally deceives or misleads others, including:
#   - Generating or promoting fraudulent content, misinformation, or defamatory statements.
#   - Distributing spam or impersonating others without consent.
#   - Misrepresenting the model's outputs as being generated by humans.
#   - Facilitating false online engagement, such as fake reviews.
#
#4. Fails to adequately inform end-users about known risks associated with your AI system.
#
### 2. Reporting Violations
#
#Violations of this Policy, as well as software bugs or security issues, should be reported through the following channels:
#- Report issues with the model: [https://github.com/ChickiLM](https://github.com/ChickiLM)
#- Report bugs and security issues: Chicki-Bugs@llm.ci
#- Report Acceptable Use Policy violations: Chicki-Viola@llm.ci
#
### 3. Disclaimer
#
#THE MODEL IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT, OR OTHERWISE, ARISING FROM, OUT OF, OR IN CONNECTION WITH THE MODEL OR THE USE OR OTHER DEALINGS IN THE MODEL.
#
### 4. Governing Law
#
#This license shall be governed by and construed in accordance with the laws of French, excluding its conflict of law provisions.
#
### 5. Termination
#
#This license is effective until terminated. Your rights under this license will terminate automatically without notice if you fail to comply with the terms. Upon termination, you must cease all use and destroy all copies of the model.
#
### 6. Contact Information
#
#For any inquiries or issues related to this license, please contact Chicki at Chicki-Contact@llm.ci
"""Configuration du modèle Chicki"""
"""Chicki model configuration"""

from transformers.configuration_utils import PretrainedConfig
from transformers.utils import logging

logger = logging.get_logger(__name__)

"""
Cette classe de configuration est utilisée pour le modèle Chicki.

Configuration class for the Chicki model.
"""
class ChickiConfig(PretrainedConfig):
    r"""
    Il s'agit de la classe de configuration pour stocker la configuration d'un [`CibleModel`]. Elle est utilisée pour instancier un modèle Cible selon les arguments spécifiés, définissant l'architecture du modèle. Instancier une configuration avec les valeurs par défaut produira une configuration similaire à celle du Cible.

    Les objets de configuration héritent de [`PretrainedConfig`] et peuvent être utilisés pour contrôler les sorties du modèle. Lisez la documentation de [`PretrainedConfig`] pour plus d'informations.

    Args:
        vocab_size (`int`, *optionnel*, par défaut à 102400):
            Taille du vocabulaire du modèle Deep. Définit le nombre de tokens différents qui peuvent être représentés par les `inputs_ids` passés lors de l'appel de [`CibleModel`].
        hidden_size (`int`, *optionnel*, par défaut à 4096):
            Dimension des représentations cachées.
        intermediate_size (`int`, *optionnel*, par défaut à 11008):
            Dimension des représentations MLP.
        moe_intermediate_size (`int`, *optionnel*, par défaut à 1407):
            Dimension des représentations MoE.
        num_hidden_layers (`int`, *optionnel*, par défaut à 32):
            Nombre de couches cachées dans le décodeur Transformer.
        num_attention_heads (`int`, *optionnel*, par défaut à 32):
            Nombre de têtes d'attention pour chaque couche d'attention dans le décodeur Transformer.
        n_shared_experts (`int`, *optionnel*, par défaut à None):
            Nombre d'experts partagés, None signifie modèle dense.
        n_routed_experts (`int`, *optionnel*, par défaut à None):
            Nombre d'experts routés, None signifie modèle dense.
        routed_scaling_factor (`float`, *optionnel*, par défaut à 1.0):
            Facteur de mise à l'échelle des experts routés.
        topk_method (`str`, *optionnel*, par défaut à `gready`):
            Méthode Topk utilisée dans la porte routée.
        n_group (`int`, *optionnel*, par défaut à None):
            Nombre de groupes pour les experts routés.
        topk_group (`int`, *optionnel*, par défaut à None):
            Nombre de groupes sélectionnés pour chaque token (pour chaque token, assurant que les experts sélectionnés sont uniquement dans les groupes `topk_group`).
        num_experts_per_tok (`int`, *optionnel*, par défaut à None):
            Nombre d'experts sélectionnés, None signifie modèle dense.
        moe_layer_freq (`int`, *optionnel*, par défaut à 1):
            Fréquence de la couche MoE : une couche d'experts pour chaque `moe_layer_freq - 1` couches denses.
        first_k_dense_replace (`int`, *optionnel*, par défaut à 0):
            Nombre de couches denses dans les couches peu profondes (embed->dense->dense->...->dense->moe->moe...->lm_head).
                                                           \--k couches denses--/
        norm_topk_prob (`bool`, *optionnel*, par défaut à False):
            S'il faut normaliser les poids des experts routés.
        scoring_func (`str`, *optionnel*, par défaut à 'softmax'):
            Méthode de calcul des poids des experts.
        aux_loss_alpha (`float`, *optionnel*, par défaut à 0.001):
            Coefficient de poids de la perte auxiliaire.
        seq_aux = (`bool`, *optionnel*, par défaut à True):
            S'il faut calculer la perte auxiliaire pour chaque échantillon individuel.
        num_key_value_heads (`int`, *optionnel*):
            Nombre de têtes de clé/valeur qui doivent être utilisées pour implémenter Grouped Query Attention. Si
            `num_key_value_heads=num_attention_heads`, le modèle utilisera Multi Head Attention (MHA), si
            `num_key_value_heads=1` le modèle utilisera Multi Query Attention (MQA), sinon GQA est utilisé. Lors
            de la conversion d'un point de contrôle multi-têtes en un point de contrôle GQA, chaque tête de clé et
            de valeur de groupe doit être construite en faisant la moyenne de toutes les têtes originales au sein de
            ce groupe. Pour plus de détails, consultez [ce document](https://github.com/DevLLM/fr.2305.13245v3/blob/main/fr.2305.13245v3.pdf). Si ce
            n'est pas spécifié, le modèle utilisera `num_attention_heads`.
        hidden_act (`str` ou `function`, *optionnel*, par défaut à `"silu"`):
            La fonction d'activation non linéaire (fonction ou chaîne) dans le décodeur.
        max_position_embeddings (`int`, *optionnel*, par défaut à 2048):
            La longueur de séquence maximale que ce modèle pourrait jamais utiliser.
        initializer_range (`float`, *optionnel*, par défaut à 0.02):
            L'écart type de l'initialisateur truncated_normal pour initialiser toutes les matrices de poids.
        rms_norm_eps (`float`, *optionnel*, par défaut à 1e-06):
            L'epsilon utilisé par les couches de normalisation rms.
        use_cache (`bool`, *optionnel*, par défaut à `True`):
            Si le modèle doit retourner ou non les dernières attentions key/values (non utilisé par tous les modèles). Seulement pertinent si `config.is_decoder=True`.
        pad_token_id (`int`, *optionnel*):
            ID du token de remplissage.
        bos_token_id (`int`, *optionnel*, par défaut à 1):
            ID du token de début de flux.
        eos_token_id (`int`, *optionnel*, par défaut à 2):
            ID du token de fin de flux.
        pretraining_tp (`int`, *optionnel*, par défaut à 1):
            Fonctionnalité expérimentale. Rang de parallélisme Tensor utilisé pendant le pré-entraînement. Veuillez consulter [ce document](https://huggingface.co/docs/transformers/parallelism) pour en savoir plus. Cette valeur est nécessaire pour assurer une reproductibilité exacte des résultats de pré-entraînement. Veuillez consulter [ce problème](https://github.com/pytorch/pytorch/issues/76232).
        tie_word_embeddings (`bool`, *optionnel*, par défaut à `False`):
            Si les poids des embeddings doivent être liés.
        rope_theta (`float`, *optionnel*, par défaut à 10000.0):
            La période de base des embeddings RoPE.
        rope_scaling (`Dict`, *optionnel*):
            Dictionnaire contenant la configuration de mise à l'échelle pour les embeddings RoPE. Prend actuellement en charge deux stratégies de mise à l'échelle : linéaire et dynamique. Leur facteur de mise à l'échelle doit être un flottant supérieur à 1. Le format attendu est `{"type": nom de la stratégie, "factor": facteur de mise à l'échelle}`. Lors de l'utilisation de ce paramètre, ne mettez pas à jour `max_position_embeddings` à la nouvelle valeur maximale attendue.
        attention_bias (`bool`, par défaut à `False`, *optionnel*, par défaut à `False`):
            S'il faut utiliser un biais dans les couches de projection query, key, value et output pendant l'auto-attention.
        attention_dropout (`float`, *optionnel*, par défaut à 0.0):
            Le ratio de dropout pour les probabilités d'attention.

    ```python
    >>> from transformers import CibleModel, CibleConfig

    >>> # Initialisation d'une configuration de style Cible
    >>> configuration = CibleConfig()

    >>> # Accéder à la configuration du modèle
    >>> configuration = model.config
    ```
    """
    r"""
    This is the configuration class to store the configuration of a [`ChickiModel`]. It is used to instantiate a
    Chicki model according to the specified arguments, defining the model architecture. Instantiating a configuration
    with the defaults will yield a similar configuration to that of
    Chicki-beta [ChickiLM/Chicki-beta](https://huggingface.co/ChickiLM/Chicki-beta).

    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the
    documentation from [`PretrainedConfig`] for more information.


    Args:
        vocab_size (`int`, *optional*, defaults to 151936):
            Vocabulary size of the Chicki model. Defines the number of different tokens that can be represented by the
            `inputs_ids` passed when calling [`ChickiModel`]
        hidden_size (`int`, *optional*, defaults to 4096):
            Dimension of the hidden representations.
        intermediate_size (`int`, *optional*, defaults to 22016):
            Dimension of the MLP representations.
        num_hidden_layers (`int`, *optional*, defaults to 32):
            Number of hidden layers in the Transformer encoder.
        num_attention_heads (`int`, *optional*, defaults to 32):
            Number of attention heads for each attention layer in the Transformer encoder.
        num_key_value_heads (`int`, *optional*, defaults to 32):
            This is the number of key_value heads that should be used to implement Grouped Query Attention. If
            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
            by meanpooling all the original heads within that group. For more details checkout [this
            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
        hidden_act (`str` or `function`, *optional*, defaults to `"silu"`):
            The non-linear activation function (function or string) in the decoder.
        max_position_embeddings (`int`, *optional*, defaults to 32768):
            The maximum sequence length that this model might ever be used with.
        initializer_range (`float`, *optional*, defaults to 0.02):
            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
        rms_norm_eps (`float`, *optional*, defaults to 1e-06):
            The epsilon used by the rms normalization layers.
        use_cache (`bool`, *optional*, defaults to `True`):
            Whether or not the model should return the last key/values attentions (not used by all models). Only
            relevant if `config.is_decoder=True`.
        tie_word_embeddings (`bool`, *optional*, defaults to `False`):
            Whether the model's input and output word embeddings should be tied.
        rope_theta (`float`, *optional*, defaults to 10000.0):
            The base period of the RoPE embeddings.
        use_sliding_window (`bool`, *optional*, defaults to `False`):
            Whether to use sliding window attention.
        sliding_window (`int`, *optional*, defaults to 4096):
            Sliding window attention (SWA) window size. If not specified, will default to `4096`.
        max_window_layers (`int`, *optional*, defaults to 28):
            The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.
        attention_dropout (`float`, *optional*, defaults to 0.0):
            The dropout ratio for the attention probabilities.

    ```python
    >>> from transformers import ChickiModel, ChickiConfig

    >>> # Initializing a Chicki style configuration
    >>> configuration = ChickiConfig()

    >>> # Initializing a model from the Chicki-7B style configuration
    >>> model = ChickiModel(configuration)

    >>> # Accessing the model configuration
    >>> configuration = model.config
    ```"""

    model_type = "chicki"
    keys_to_ignore_at_inference = ["past_key_values"]

    def __init__(
        self,
        vocab_size=151936,
        hidden_size=4096,
        intermediate_size=22016,
        num_hidden_layers=32,
        num_attention_heads=32,
        num_key_value_heads=32,
        hidden_act="silu",
        max_position_embeddings=32768,
        use_sliding_window=False,
        sliding_window=4096,
        max_window_layers=28,
        initializer_range=0.02,
        rms_norm_eps=1e-6,
        use_cache=True,
        rope_theta=10000.0,
        attention_dropout=0.0,
        tie_word_embeddings=False,
        **kwargs,
    ):
        """
        Initialise la configuration de Chicki.

        Initialize the Chicki configuration.
        """
        # Paramètres de l'architecture du modèle
        # Model architecture parameters
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.num_key_value_heads = num_key_value_heads or num_attention_heads
        self.hidden_act = hidden_act
        self.max_position_embeddings = max_position_embeddings

        # Paramètres de l'attention à fenêtre glissante
        # Sliding window attention parameters
        self.use_sliding_window = use_sliding_window
        self.sliding_window = sliding_window if use_sliding_window else None
        self.max_window_layers = max_window_layers

        # Paramètres d'initialisation et de normalisation
        # Initialization and normalization parameters
        self.initializer_range = initializer_range
        self.rms_norm_eps = rms_norm_eps

        # Autres paramètres du modèle
        # Other model settings
        self.use_cache = use_cache
        self.rope_theta = rope_theta
        self.attention_dropout = attention_dropout

        super().__init__(
            tie_word_embeddings=tie_word_embeddings,
            **kwargs,
        )

    # @property
    # def head_dim(self):
    #     """
    #     Renvoie la dimension de chaque tête d'attention.

    #     Return the dimension of each attention head.
    #     """
    #     return self.hidden_size // self.num_attention_heads

    # @property
    # def num_key_value_groups(self):
    #     """
    #     Renvoie le nombre de groupes clé/valeur pour l'attention à requête groupée.

    #     Return the number of key/value groups for grouped query attention.
    #     """
    #     return self.num_attention_heads // self.num_key_value_heads

    # def to_dict(self):
    #     """
    #     Convertit la configuration en un dictionnaire.

    #     Convert the configuration to a dictionary.
    #     """
    #     output = super().to_dict()
    #     output["head_dim"] = self.head_dim
    #     output["num_key_value_groups"] = self.num_key_value_groups
    #     return output

