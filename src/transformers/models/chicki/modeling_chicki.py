# coding=utf-8
## Licence d'Utilisation Chicki
#
#Copyright (c) 2024 Chicki. Tous droits réservés.
#
#Cette licence régit l'utilisation du modèle Chicki. En utilisant, copiant ou distribuant ce modèle, vous acceptez de vous conformer aux termes et conditions suivants.
#
### 1. Politique d'Utilisation Acceptable
#
#Vous devez respecter la Politique d'Utilisation Acceptable de Chicki comme détaillée ci-dessous. Cela inclut des restrictions sur les activités illégales, les comportements nuisibles, et l'utilisation abusive du modèle.
#
#### 1.1 Usages Interdits
#Vous vous engagez à ne pas utiliser le modèle, ni à permettre à d'autres de l'utiliser, pour toute finalité qui :
#
#1. Violerait une loi ou porterait atteinte aux droits d'autrui, y compris :
#   - Participer à ou promouvoir des activités illégales telles que la violence, le terrorisme, la traite des êtres humains, l'exploitation de mineurs, ou tout autre acte illicite.
#   - Faciliter le harcèlement, les abus, la discrimination, ou l'exercice non autorisé de toute profession.
#   - Collecter ou traiter des données personnelles sensibles sans autorisation appropriée.
#   - Créer ou distribuer des logiciels malveillants ou perturber autrement les systèmes numériques.
#
#2. Impliquerait des activités présentant un risque de préjudice physique ou de mort pour des individus, telles que :
#   - Applications liées à des activités militaires, nucléaires, ou d'espionnage.
#   - Développement ou utilisation d'armes illégales, de drogues ou de substances contrôlées.
#   - Exploitation d'infrastructures critiques, de technologies de transport ou de machines lourdes.
#   - Promouvoir l'automutilation, le suicide ou toute autre forme de préjudice corporel.
#
#3. Tromperait ou induirait intentionnellement en erreur les autres, y compris :
#   - Générer ou promouvoir du contenu frauduleux, de la désinformation, ou des déclarations diffamatoires.
#   - Distribuer du spam ou se faire passer pour autrui sans consentement.
#   - Représenter à tort que les sorties du modèle sont générées par des humains.
#   - Faciliter un engagement en ligne faux, comme de fausses critiques.
#
#4. Ne divulgue pas de manière adéquate aux utilisateurs finaux les risques connus associés à votre système d'IA.
#
### 2. Signalement des Violations
#
#Les violations de cette Politique, ainsi que les bugs logiciels ou les problèmes de sécurité, doivent être signalés via les canaux suivants :
#- Signaler des problèmes avec le modèle : [https://github.com/ChickiLM](https://github.com/ChickiLM)
#- Signaler des bugs et des problèmes de sécurité : Chicki-Bugs@llm.ci
#- Signaler des violations de la Politique d'Utilisation Acceptable : Chicki-Viola@llm.ci
#
### 3. Avertissement
#
#LE MODÈLE EST FOURNI "TEL QUEL", SANS AUCUNE GARANTIE, EXPRESSE OU IMPLICITE, Y COMPRIS MAIS SANS S'Y LIMITER AUX GARANTIES DE QUALITÉ MARCHANDE, D'ADÉQUATION À UN USAGE PARTICULIER, ET DE NON-VIOLATION. EN AUCUN CAS LE DÉTENTEUR DU DROIT D'AUTEUR OU LES CONTRIBUTEURS NE SERONT RESPONSABLES DE TOUTE RÉCLAMATION, DOMMAGE OU AUTRE RESPONSABILITÉ, QUE CE SOIT DANS LE CADRE D'UN CONTRAT, D'UN DÉLIT OU AUTREMENT, DÉCOULANT DE, DEPUIS OU EN LIEN AVEC LE MODÈLE OU L'UTILISATION OU AUTRES INTERACTIONS AVEC LE MODÈLE.
#
### 4. Droit Applicable
#
#Cette licence est régie et interprétée conformément aux lois françaises, à l'exclusion de ses dispositions relatives aux conflits de lois.
#
### 5. Résiliation
#
#Cette licence est effective jusqu'à sa résiliation. Vos droits en vertu de cette licence seront automatiquement résiliés sans préavis si vous ne respectez pas les termes. À la résiliation, vous devez cesser toute utilisation et détruire toutes les copies du modèle.
#
### 6. Informations de Contact
#
#Pour toute question ou problème lié à cette licence, veuillez contacter Chicki à Chicki-Contact@llm.ci.
#
#
#
## Chicki License Agreement
#
#Copyright (c) 2024 Chicki. All rights reserved.
#
#This license governs the use of the Chicki model. By using, copying, or distributing this model, you agree to comply with the following terms and conditions.
#
### 1. Acceptable Use Policy
#
#You must comply with the Chicki Acceptable Use Policy as detailed below. This includes restrictions on illegal activities, harmful behavior, and misuse of the model.
#
#### 1.1 Prohibited Uses
#You agree not to use the model, nor allow others to use it, for any purpose that:
#
#1. Violates any law or infringes the rights of others, including:
#   - Engaging in or promoting illegal activities such as violence, terrorism, human trafficking, exploitation of minors, or any other unlawful act.
#   - Facilitating harassment, abuse, discrimination, or unauthorized practice of any profession.
#   - Collecting or processing sensitive personal data without proper authorization.
#   - Creating or distributing malicious software or otherwise disrupting digital systems.
#
#2. Involves activities that pose a risk of physical harm or death to individuals, such as:
#   - Applications related to military, nuclear, or espionage activities.
#   - Development or use of illegal weapons, drugs, or controlled substances.
#   - Exploitation of critical infrastructure, transportation technologies, or heavy machinery.
#   - Promoting self-harm, suicide, or other forms of bodily harm.
#
#3. Intentionally deceives or misleads others, including:
#   - Generating or promoting fraudulent content, misinformation, or defamatory statements.
#   - Distributing spam or impersonating others without consent.
#   - Misrepresenting the model's outputs as being generated by humans.
#   - Facilitating false online engagement, such as fake reviews.
#
#4. Fails to adequately inform end-users about known risks associated with your AI system.
#
### 2. Reporting Violations
#
#Violations of this Policy, as well as software bugs or security issues, should be reported through the following channels:
#- Report issues with the model: [https://github.com/ChickiLM](https://github.com/ChickiLM)
#- Report bugs and security issues: Chicki-Bugs@llm.ci
#- Report Acceptable Use Policy violations: Chicki-Viola@llm.ci
#
### 3. Disclaimer
#
#THE MODEL IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT, OR OTHERWISE, ARISING FROM, OUT OF, OR IN CONNECTION WITH THE MODEL OR THE USE OR OTHER DEALINGS IN THE MODEL.
#
### 4. Governing Law
#
#This license shall be governed by and construed in accordance with the laws of French, excluding its conflict of law provisions.
#
### 5. Termination
#
#This license is effective until terminated. Your rights under this license will terminate automatically without notice if you fail to comply with the terms. Upon termination, you must cease all use and destroy all copies of the model.
#
### 6. Contact Information
#
#For any inquiries or issues related to this license, please contact Chicki at Chicki-Contact@llm.ci
"""Modèle PyTorch Chicki."""
"""PyTorch Chicki model."""

import math
from typing import List, Optional, Tuple, Union

import torch
import torch.utils.checkpoint
from torch import nn
from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss

from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, DynamicCache, StaticCache
from transformers.modeling_attn_mask_utils import AttentionMaskConverter
from transformers.modeling_outputs import (
    BaseModelOutputWithPast,
    CausalLMOutputWithPast,
    SequenceClassifierOutputWithPast,
    TokenClassifierOutput,
)
from transformers.modeling_utils import PreTrainedModel
from transformers.utils import (
    add_start_docstrings,
    add_start_docstrings_to_model_forward,
    is_flash_attn_2_available,
    is_flash_attn_greater_or_equal_2_10,
    logging,
    replace_return_docstrings,
)
from configuration_chicki import ChickiConfig

# Vérifier si Flash Attention 2 est disponible
# Check if Flash Attention 2 is available
if is_flash_attn_2_available():
    from transformers.modeling_flash_attention_utils import _flash_attention_forward

# Initialiser le logger
# Initialize logger
logger = logging.get_logger(__name__)

# Définir les constantes pour le modèle Chicki
# Define constants for Chicki model
CHICKI_PRETRAINED_MODEL_ARCHIVE_LIST = [
    "ChickiLM/Chicki-tokenizer",
    # Voir tous les modèles Chicki sur https://huggingface.co/models?filter=chicki
    # See all Chicki models at https://huggingface.co/models?filter=chicki
]

_CHECKPOINT_FOR_DOC = "ChickiLM/Chicki-beta"
_CONFIG_FOR_DOC = "ChickiConfig"

# Copié de transformers.models.llama.modeling_llama._prepare_4d_causal_attention_mask_with_cache_position
# Copied from transformers.models.llama.modeling_llama._prepare_4d_causal_attention_mask_with_cache_position
def _prepare_4d_causal_attention_mask_with_cache_position(
    attention_mask: torch.Tensor,
    sequence_length: int,
    target_length: int,
    dtype: torch.dtype,
    device: torch.device,
    min_dtype: float,
    cache_position: torch.Tensor,
    batch_size: int,
):
    """
    Crée un masque causal 4D de forme `(batch_size, 1, query_length, key_value_length)` à partir d'un masque 2D de forme
    `(batch_size, key_value_length)`, ou si l'entrée `attention_mask` est déjà 4D, ne fait rien.

    Args:
        attention_mask (`torch.Tensor`):
            Un masque d'attention 2D de forme `(batch_size, key_value_length)` ou un masque d'attention 4D de forme `(batch_size, 1, query_length, key_value_length)`.
        sequence_length (`int`):
            La longueur de la séquence en cours de traitement.
        target_length (`int`):
            La longueur cible: lors de la génération avec un cache statique, le masque doit être aussi long que le cache statique, pour tenir compte du remplissage 0, la partie du cache qui n'est pas encore remplie.
        dtype (`torch.dtype`):
            Le dtype à utiliser pour le masque d'attention 4D.
        device (`torch.device`):
            Le périphérique sur lequel placer le masque d'attention 4D.
        min_dtype (`float`):
            La valeur minimale représentable avec le dtype `dtype`.
        cache_position (`torch.Tensor`):
            Indices décrivant la position des jetons de séquence d'entrée dans la séquence.
        batch_size (`torch.Tensor`):
            Taille du lot.

    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape
    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.

    Args:
        attention_mask (`torch.Tensor`):
            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.
        sequence_length (`int`):
            The sequence length being processed.
        target_length (`int`):
            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.
        dtype (`torch.dtype`):
            The dtype to use for the 4D attention mask.
        device (`torch.device`):
            The device to plcae the 4D attention mask on.
        min_dtype (`float`):
            The minimum value representable with the dtype `dtype`.
        cache_position (`torch.Tensor`):
            Indices depicting the position of the input sequence tokens in the sequence.
        batch_size (`torch.Tensor`):
            Batch size.
    """
    # Vérifier si le masque d'entrée est déjà en 4D
    # Check if the input mask is already 4D
    if attention_mask is not None and attention_mask.dim() == 4:
        # Si c'est le cas, retourner le masque tel quel
        # If so, return the mask as is
        causal_mask = attention_mask
    else:
        # Initialiser un masque causal avec la valeur minimale du type de données
        # Initialize a causal mask with the minimum value of the data type
        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)
        # Si la séquence n'est pas de longueur 1, appliquer un masque triangulaire supérieur
        # If the sequence is not of length 1, apply an upper triangular mask
        if sequence_length != 1:
            causal_mask = torch.triu(causal_mask, diagonal=1)
        # Ajuster le masque en fonction de la position du cache
        # Adjust the mask based on the cache position
        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)
        # Étendre le masque pour correspondre aux dimensions du batch et des têtes d'attention
        # Expand the mask to match the batch and attention head dimensions    
        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)
        # Si un masque d'attention est fourni, l'intégrer au masque causal
        # If an attention mask is provided, incorporate it into the causal mask    
        if attention_mask is not None:
            # Copier le masque causal dans une mémoire contiguë pour une édition en place
            # Copy the causal mask to contiguous memory for in-place editing        
            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit
            # Obtenir la longueur du masque d'attention
            # Get the length of the attention mask    
            mask_length = attention_mask.shape[-1]
            # Combiner le masque causal avec le masque d'attention
            # Combine the causal mask with the attention mask        
            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]
            padding_mask = padding_mask == 0
            # Mettre à jour le masque causal avec les valeurs du masque de remplissage
            # Update the causal mask with the padding mask values        
            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(
                padding_mask, min_dtype
            )
            
    # Retourner le masque causal final
    # Return the final causal mask
    return causal_mask

# Copié de transformers.models.llama.modeling_llama.LlamaRMSNorm avec Llama->Chicki
# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Chicki
class ChickiRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        ChickiRMSNorm est équivalent à T5LayerNorm
        ChickiRMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        # Initialise un paramètre de poids avec des valeurs de 1 pour chaque dimension cachée
        # Initialize a weight parameter with values of 1 for each hidden dimension
        self.weight = nn.Parameter(torch.ones(hidden_size))
        # Définit un petit epsilon pour éviter la division par zéro
        # Set a small epsilon to avoid division by zero
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        # Sauvegarde le type de données d'entrée pour la conversion finale
        # Save the input data type for final conversion
        input_dtype = hidden_states.dtype
        # Convertit les états cachés en float32 pour des calculs précis
        # Convert hidden states to float32 for precise calculations
        hidden_states = hidden_states.to(torch.float32)
        # Calcule la variance le long de la dernière dimension
        # Calculate variance along the last dimension
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        # Normalise les états cachés en utilisant la racine carrée inverse de la variance
        # Normalize hidden states using the inverse square root of variance
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        # Applique le poids et reconvertit au type de données d'entrée
        # Apply weight and convert back to input data type
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        # Fournit une représentation string supplémentaire pour le module
        # Provide additional string representation for the module
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"

# Copié de transformers.models.mixtral.modeling_mixtral.MixtralRotaryEmbedding avec Mixtral->Chicki
# Copied from transformers.models.mixtral.modeling_mixtral.MixtralRotaryEmbedding with Mixtral->Chicki
class ChickiRotaryEmbedding(nn.Module):
    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()

        # Initialisation des paramètres de l'embedding rotatif
        # Initialization of rotary embedding parameters
        self.dim = dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base

        # Calcul des fréquences inverses pour l'embedding rotatif
        # Calculation of inverse frequencies for rotary embedding
        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)

        # Initialisation du cache cosinus et sinus
        # Initialization of cosine and sine cache
        self._set_cos_sin_cache(
            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()
        )

    def _set_cos_sin_cache(self, seq_len, device, dtype):
        # Mise à jour du cache cosinus et sinus pour une longueur de séquence donnée
        # Update cosine and sine cache for a given sequence length
        self.max_seq_len_cached = seq_len
        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)

        # Calcul des fréquences et création de l'embedding
        # Calculation of frequencies and creation of the embedding
        freqs = torch.outer(t, self.inv_freq)
        # Concaténation des fréquences pour obtenir l'embedding complet
        # Concatenation of frequencies to get the complete embedding
        emb = torch.cat((freqs, freqs), dim=-1)

        # Enregistrement des caches cosinus et sinus
        # Registration of cosine and sine caches
        self.register_buffer("cos_cached", emb.cos().to(dtype), persistent=False)
        self.register_buffer("sin_cached", emb.sin().to(dtype), persistent=False)

    def forward(self, x, seq_len=None):
        # Fonction forward pour appliquer l'embedding rotatif
        # Forward function to apply rotary embedding
        # x: [bs, num_attention_heads, seq_len, head_size]
        if seq_len > self.max_seq_len_cached:
            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)

        return (
            self.cos_cached[:seq_len].to(dtype=x.dtype),
            self.sin_cached[:seq_len].to(dtype=x.dtype),
        )

# Copié de transformers.models.llama.modeling_llama.rotate_half
# Copied from transformers.models.llama.modeling_llama.rotate_half
def rotate_half(x):
    """
    Fait pivoter la moitié des dimensions cachées de l'entrée.
    Rotates half the hidden dims of the input.
    """    
    # Divise le tenseur d'entrée en deux moitiés le long de la dernière dimension    
    # Split the input tensor into two halves along the last dimension
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    # Concatène la deuxième moitié négative avec la première moitié
    # Cela fait effectivement pivoter la représentation    
    # Concatenate the second half negated with the first half
    # This effectively rotates the representation
    return torch.cat((-x2, x1), dim=-1)

# Copié de transformers.models.mixtral.modeling_mixtral.apply_rotary_pos_emb
# Copied from transformers.models.mixtral.modeling_mixtral.apply_rotary_pos_emb
def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):
    """
    Applies Rotary Position Embedding to the query and key tensors.
    Applique l'Embedding de Position Rotatif aux tenseurs de requête et de clé.

    Args:
        q (torch.Tensor): The query tensor. / Le tenseur de requête.
        k (torch.Tensor): The key tensor. / Le tenseur de clé.
        cos (torch.Tensor): The cosine part of the rotary embedding. / La partie cosinus de l'embedding rotatif.
        sin (torch.Tensor): The sine part of the rotary embedding. / La partie sinus de l'embedding rotatif.
        position_ids (torch.Tensor): The position indices of the tokens. / Les indices de position des tokens.
        unsqueeze_dim (int, optional): The dimension to unsqueeze. / La dimension à étendre.

    Returns:
        tuple(torch.Tensor): The rotated query and key tensors. / Les tenseurs de requête et de clé rotés.
    """
    # Extrait les valeurs de cosinus et de sinus pour les IDs de position donnés    
    # Extract the cosine and sine values for the given position IDs
    cos = cos[position_ids].unsqueeze(unsqueeze_dim)
    sin = sin[position_ids].unsqueeze(unsqueeze_dim)
    # Applique la rotation au tenseur de requête    
    # Apply the rotation to the query tensor
    q_embed = (q * cos) + (rotate_half(q) * sin)
    # Applique la rotation au tenseur de clé    
    # Apply the rotation to the key tensor
    k_embed = (k * cos) + (rotate_half(k) * sin)
    # Retourne les tenseurs de requête et de clé rotés    
    # Return the rotated query and key tensors
    return q_embed, k_embed

# Copié de transformers.models.mistral.modeling_mistral.MistralMLP avec Mistral->Chicki
# Copied from transformers.models.mistral.modeling_mistral.MistralMLP with Mistral->Chicki
class ChickiMLP(nn.Module):
    """
    ChickiMLP implémente un Perceptron Multicouche pour le modèle Chicki.
    ChickiMLP implements a Multi-Layer Perceptron for the Chicki model.
    """
    def __init__(self, config):
        """
        Initialise le ChickiMLP avec la configuration donnée.
        Initialize the ChickiMLP with the given configuration.
        """
        super().__init__()
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        
        # Projections linéaires pour les transformations de porte, montée et descente
        # Linear projections for gate, up, and down transformations
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        
        # Fonction d'activation spécifiée dans la configuration
        # Activation function specified in the configuration
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, hidden_state):
        """
        Passage avant du ChickiMLP.
        Forward pass of the ChickiMLP.
        
        Le passage avant consiste en :
        1. Application des projections de porte et de montée à l'entrée
        2. Application de la fonction d'activation à la projection de porte
        3. Multiplication élément par élément des projections de porte activée et de montée
        4. Application de la projection de descente au résultat
        
        The forward pass consists of:
        1. Applying gate and up projections to the input
        2. Applying the activation function to the gate projection
        3. Element-wise multiplication of activated gate and up projections
        4. Applying the down projection to the result
        """
        return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))

# Copié de transformers.models.llama.modeling_llama.repeat_kv
# Copied from transformers.models.llama.modeling_llama.repeat_kv
def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    Cette fonction répète les états cachés des clés et des valeurs pour l'attention multi-têtes.
    Elle est équivalente à torch.repeat_interleave(x, dim=1, repeats=n_rep).
    Les états cachés sont transformés de (batch, num_key_value_heads, seqlen, head_dim)
    à (batch, num_attention_heads, seqlen, head_dim).

    This function repeats the key and value hidden states for multi-head attention.
    It's equivalent to torch.repeat_interleave(x, dim=1, repeats=n_rep).
    The hidden states are transformed from (batch, num_key_value_heads, seqlen, head_dim)
    to (batch, num_attention_heads, seqlen, head_dim).
    """
    
    # Extraire les dimensions du tenseur d'entrée
    # Extract dimensions of the input tensor
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    
    # Si n_rep est 1, aucune répétition n'est nécessaire, retourner l'entrée telle quelle
    # If n_rep is 1, no repetition is needed, return the input as is
    if n_rep == 1:
        return hidden_states
    
    # Étendre le tenseur le long d'une nouvelle dimension et le répéter n_rep fois
    # Expand the tensor along a new dimension and repeat it n_rep times
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    
    # Remodeler le tenseur pour combiner les dimensions num_key_value_heads et n_rep
    # Reshape the tensor to combine the num_key_value_heads and n_rep dimensions
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)

class ChickiAttention(nn.Module):
    """
    Attention multi-têtes tirée de l'article « Attention Is All You Need ». 
    Modifié pour utiliser l'attention de la fenêtre coulissante: Longformer 
    et "Générer de longues séquences avec des transformateurs clairsemés".
    
    Multi-headed attention from 'Attention Is All You Need' paper. 
    Modified to use sliding window attention: Longformer
    and "Generating Long Sequences with Sparse Transformers".
    """

    # Initialisation de la classe avec la configuration et l'index de la couche optionnel
    # Initialization of the class with the configuration and optional layer index
    def __init__(self, config: ChickiConfig, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        # Avertissement si l'index de la couche n'est pas fourni
        # Warning if the layer index is not provided    
        if layer_idx is None:
            logger.warning_once(
                f"Instantiating {self.__class__.__name__} without passing `layer_idx` is not recommended and will "
                "to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` "
                "when creating this class."
            )

        # Configuration des paramètres de l'attention
        # Setting up attention parameters
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.max_position_embeddings = config.max_position_embeddings
        self.rope_theta = config.rope_theta
        self.is_causal = True
        self.attention_dropout = config.attention_dropout

        # Vérification de la compatibilité des dimensions
        # Checking dimension compatibility    
        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )
            
        # Initialisation des projections linéaires pour les requêtes, clés, valeurs et sortie
        # Initializing linear projections for queries, keys, values, and output    
        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)
        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)

        # Initialisation de l'embedding rotatif (RoPE)
        # Initializing rotary positional embedding (RoPE)    
        self.rotary_emb = ChickiRotaryEmbedding(
            self.head_dim,
            max_position_embeddings=self.max_position_embeddings,
            base=self.rope_theta,
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        bsz, q_len, _ = hidden_states.size()

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        kv_seq_len = key_states.shape[-2]
        if past_key_value is not None:
            if self.layer_idx is None:
                raise ValueError(
                    f"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} "
                    "for auto-regressive decoding with k/v caching, please make sure to initialize the attention class "
                    "with a layer index."
                )
            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        # repeat k/v heads if n_kv_heads < n_heads
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)
        # Calcul des poids d'attention
        # Calculation of attention weights
        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)
        # Vérification de la taille des poids d'attention
        # Verification of the attention weights size
        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
            raise ValueError(
                f"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is"
                f" {attn_weights.size()}"
            )
        # Application du masque d'attention si fourni
        # Application of attention mask if provided
        if attention_mask is not None:
            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
            attn_weights = attn_weights + causal_mask

        # attention portée à fp32
        # upcast attention to fp32
        
        # Conversion des poids d'attention en float32 et application de softmax
        # Conversion of attention weights to float32 and application of softmax
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
        # Application du dropout sur les poids d'attention
        # Application of dropout on attention weights
        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)
        # Calcul de la sortie d'attention
        # Calculation of attention output
        attn_output = torch.matmul(attn_weights, value_states)
        
        # Vérification de la taille de la sortie d'attention
        # Verification of the attention output size
        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )
            
        # Réorganisation de la sortie d'attention
        # Reorganization of attention output
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
        # Projection finale de la sortie d'attention
        # Final projection of attention output
        attn_output = self.o_proj(attn_output)
        # Gestion de la sortie des poids d'attention si nécessaire
        # Handling of attention weights output if needed
        if not output_attentions:
            attn_weights = None

        # Retour de la sortie d'attention, des poids d'attention et du cache
        # Return of attention output, attention weights, and cache
        return attn_output, attn_weights, past_key_value

class ChickiFlashAttention2(ChickiAttention):
    """
    Chicki flash attention module, following Chicki attention module. This module inherits from `ChickiAttention`
    as the weights of the module stays untouched. The only required change would be on the forward pass
    where it needs to correctly call the public API of flash attention and deal with padding tokens
    in case the input contains any of them. Additionally, for sliding window attention, we apply SWA only to the bottom
    config.max_window_layers layers.
    """

    # Copié de transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__
    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # TODO: À supprimer une fois que Flash Attention pour RoCm sera mis à jour vers la version 2.1.
        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.
        
        # flash_attn<2.1 génère un masque causal aligné en haut à gauche, alors que ce qui est nécessaire ici est un alignement en bas à droite, 
        # qui est devenu la valeur par défaut pour flash_attn>=2.1. Cet attribut est utilisé pour gérer cette différence.
        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, 
        # that was made default for flash_attn>=2.1. This attribute is used to handle this difference.
        
        # Référence: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0
        # Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0
        
        # Attention: avec flash_attn<2.1, l'utilisation de q_seqlen != k_seqlen (sauf pour le cas q_seqlen == 1) produit un masque incorrect (en haut à gauche).
        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).
        
        # Définit un attribut pour vérifier si la version de flash_attn utilisée génère un masque aligné en haut à gauche
        # Set an attribute to check if the used flash_attn version generates a top-left aligned mask
        
        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()

    # Fonction forward pour le traitement de l'attention
    # Forward function for processing attention
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
    ):
        # Obtenir les dimensions du tenseur d'entrée
        # Get the dimensions of the input tensor    
        bsz, q_len, _ = hidden_states.size()

        # Projeter les états cachés pour obtenir les états de requête, clé et valeur
        # Project hidden states to get query, key, and value states    
        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        # Redimensionner et transposer les états pour le traitement multi-têtes
        # Reshape and transpose states for multi-head processing
        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        # Calculer la longueur de la séquence clé-valeur
        # Calculate the key-value sequence length
        kv_seq_len = key_states.shape[-2]
        if past_key_value is not None:
            if self.layer_idx is None:
                raise ValueError(
                    f"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} "
                    "for auto-regressive decoding with k/v caching, please make sure to initialize the attention class "
                    "with a layer index."
                )
            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)

        # Calculer la longueur de séquence rotationnelle
        # Calculate the rotary sequence length    
        rotary_seq_len = (
            max(kv_seq_len, position_ids[:, -1].max().item() + 1) if position_ids is not None else kv_seq_len
        )

        # Obtenir les embeddings rotationnels
        # Get rotary embeddings
        cos, sin = self.rotary_emb(value_states, seq_len=rotary_seq_len)
        # Appliquer les embeddings de position rotationnels
        # Apply rotary positional embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

        # Gérer le cache des clés et valeurs passées
        # Handle past key-value cache
        if past_key_value is not None:
            # Activer le cache de découpage si nécessaire
            # Activate slicing cache if needed
            cache_has_contents = past_key_value.get_seq_length(self.layer_idx) > 0
            if (
                getattr(self.config, "sliding_window", None) is not None
                and kv_seq_len > self.config.sliding_window
                and cache_has_contents
            ):
                slicing_tokens = 1 - self.config.sliding_window

                past_key = past_key_value[self.layer_idx][0]
                past_value = past_key_value[self.layer_idx][1]

                past_key = past_key[:, :, slicing_tokens:, :].contiguous()
                past_value = past_value[:, :, slicing_tokens:, :].contiguous()

                if past_key.shape[-2] != self.config.sliding_window - 1:
                    raise ValueError(
                        f"past key must have a shape of (`batch_size, num_heads, self.config.sliding_window-1, head_dim`), got"
                        f" {past_key.shape}"
                    )

                if attention_mask is not None:
                    attention_mask = attention_mask[:, slicing_tokens:]
                    attention_mask = torch.cat([attention_mask, torch.ones_like(attention_mask[:, -1:])], dim=-1)

            # Mettre à jour le cache des clés et valeurs
            # Update key-value cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        # répéter les têtes clé/valeur si nécessaire
        # repeat k/v heads if n_kv_heads < n_heads
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)
        # Définir le taux de dropout
        # Set dropout rate    
        dropout_rate = 0.0 if not self.training else self.attention_dropout

        # Gérer les problèmes de casting de type
        # Handle type casting issues    
        input_dtype = query_states.dtype
        if input_dtype == torch.float32:
            if torch.is_autocast_enabled():
                target_dtype = torch.get_autocast_gpu_dtype()
            elif hasattr(self.config, "_pre_quantization_dtype"):
                target_dtype = self.config._pre_quantization_dtype
            else:
                target_dtype = self.q_proj.weight.dtype

            logger.warning_once(
                f"The input hidden states seems to be silently casted in float32, this might be related to"
                f" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in"
                f" {target_dtype}."
            )

            query_states = query_states.to(target_dtype)
            key_states = key_states.to(target_dtype)
            value_states = value_states.to(target_dtype)
            
        # Réorganiser les tenseurs pour Flash Attention
        # Reshape tensors for Flash Attention
        query_states = query_states.transpose(1, 2)
        key_states = key_states.transpose(1, 2)
        value_states = value_states.transpose(1, 2)
        
        # Configurer la fenêtre glissante si nécessaire
        # Set up sliding window if needed
        if (
            self.config.use_sliding_window
            and getattr(self.config, "sliding_window", None) is not None
            and self.layer_idx >= self.config.max_window_layers
        ):
            sliding_window = self.config.sliding_window
        else:
            sliding_window = None

        # Appliquer Flash Attention
        # Apply Flash Attention
        attn_output = _flash_attention_forward(
            query_states,
            key_states,
            value_states,
            attention_mask,
            q_len,
            position_ids=position_ids,
            dropout=dropout_rate,
            sliding_window=sliding_window,
            is_causal=self.is_causal,
            use_top_left_mask=self._flash_attn_uses_top_left_mask,
        )

        # Réorganiser et projeter la sortie d'attention
        # Reshape and project attention output
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()
        attn_output = self.o_proj(attn_output)

        # Gérer la sortie des poids d'attention si nécessaire
        # Handle attention weights output if needed
        if not output_attentions:
            attn_weights = None

        # Retourner la sortie d'attention, les poids d'attention et le cache
        # Return attention output, attention weights, and cache
        return attn_output, attn_weights, past_key_value

# Copié de transformers.models.mixtral.modeling_mixtral.MixtralSdpaAttention avec Mixtral->Chicki
# Copied from transformers.models.mixtral.modeling_mixtral.MixtralSdpaAttention with Mixtral->Chicki
class ChickiSdpaAttention(ChickiAttention):
    """
    Module d'attention Chicki utilisant torch.nn.functional.scaled_dot_product_attention. Ce module hérite de 
    `ChickiAttention` car les poids du module restent inchangés. Les seuls changements concernent le passage en avant pour s'adapter 
    à l'API SDPA.
    
    Chicki attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from
    `ChickiAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to
    SDPA API.
    """

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        # Si output_attentions est vrai, on utilise l'implémentation manuelle de l'attention
        # If output_attentions is true, we use the manual implementation of attention    
        if output_attentions:
            logger.warning_once(
                "ChickiModel is using ChickiSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, "
                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.'
            )
            return super().forward(
                hidden_states=hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=past_key_value,
                output_attentions=output_attentions,
                use_cache=use_cache,
            )
        # Récupération des dimensions du tenseur d'entrée
        # Get the dimensions of the input tensor
        bsz, q_len, _ = hidden_states.size()

        # Projection des états cachés pour obtenir les requêtes, clés et valeurs
        # Project hidden states to get queries, keys, and values    
        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        # Reshape et transposition des tenseurs pour le calcul de l'attention
        # Reshape and transpose tensors for attention calculation    
        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        # Calcul de la longueur de séquence pour les clés et valeurs
        # Calculate sequence length for keys and values    
        kv_seq_len = key_states.shape[-2]
        if past_key_value is not None:
            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
        # Application de l'encodage de position rotatif (RoPE)
        # Apply rotary positional encoding (RoPE)
        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)

        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

        # Gestion du cache pour l'inférence efficace
        # Handle cache for efficient inference    
        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        # Répétition des clés et valeurs pour le multi-head attention
        # Repeat keys and values for multi-head attention    
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        # Préparation du masque d'attention causal
        # Prepare causal attention mask    
        causal_mask = attention_mask
        if attention_mask is not None:
            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]

        # Gestion des problèmes de contiguïté pour CUDA
        # Handle contiguity issues for CUDA    
        if query_states.device.type == "cuda" and attention_mask is not None:
            query_states = query_states.contiguous()
            key_states = key_states.contiguous()
            value_states = value_states.contiguous()

        is_causal = True if causal_mask is None and q_len > 1 else False
        
        # Calcul de l'attention avec scaled dot-product attention
        # Compute attention using scaled dot-product attention
        attn_output = torch.nn.functional.scaled_dot_product_attention(
            query_states,
            key_states,
            value_states,
            attn_mask=causal_mask,
            dropout_p=self.attention_dropout if self.training else 0.0,
            is_causal=is_causal,
        )

        # Reshape et projection de la sortie de l'attention
        # Reshape and project attention output    
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(bsz, q_len, self.hidden_size)
        attn_output = self.o_proj(attn_output)

        return attn_output, None, past_key_value

CHICKI_ATTENTION_CLASSES = {
    "eager": ChickiAttention,
    "flash_attention_2": ChickiFlashAttention2,
    "sdpa": ChickiSdpaAttention,
}

class ChickiDecoderLayer(nn.Module):
    def __init__(self, config: ChickiConfig, layer_idx: int):
        super().__init__()
        # Définir la taille cachée à partir de la configuration
        # Set the hidden size from the configuration    
        self.hidden_size = config.hidden_size
        
        # Vérifier si l'attention à fenêtre glissante est activée mais non implémentée
        # Check if sliding window attention is enabled but not implemented
        if config.sliding_window and config._attn_implementation != "flash_attention_2":
            logger.warning_once(
                f"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; "
                "unexpected results may be encountered."
            )
        # Initialiser le mécanisme d'attention en fonction de la configuration
        # Initialize the attention mechanism based on the configuration        
        self.self_attn = CHICKI_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)

        # Initialiser le MLP (Multi-Layer Perceptron)
        # Initialize the MLP (Multi-Layer Perceptron)
        self.mlp = ChickiMLP(config)
        # Initialiser les couches de normalisation d'entrée et post-attention
        # Initialize input and post-attention normalization layers        
        self.input_layernorm = ChickiRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = ChickiRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs,
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
        """
        Args :
            hidden_states (`torch.FloatTensor`) : entrée dans la couche de forme `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`, *optional*) : masque d'attention de taille
                `(batch, sequence_length)` où les éléments de remplissage sont indiqués par 0.
            output_attentions (`bool`, *optional*) :
                Indique s'il faut ou non renvoyer les tenseurs d'attention de toutes les couches d'attention. Voir `attentions` sous
                les tenseurs renvoyés pour plus de détails.
            use_cache (`bool`, *optional*) :
                Si défini sur `True`, les états de valeur clé `past_key_values` sont renvoyés et peuvent être utilisés pour accélérer le décodage
                (voir `past_key_values`).
            past_key_value (`Tuple(torch.FloatTensor)`, *optional*) : passé mis en cache États de projection de clé et de valeur
            cache_position (`torch.LongTensor` de forme `(sequence_length)`, *optional*):
                Indices décrivant la position des jetons de séquence d'entrée dans la séquence.
            kwargs (`dict`, *optional*):
                Kwargs arbitraires à ignorer, utilisés pour FSDP et d'autres méthodes qui injectent du code
                dans le modèle
        
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size
                `(batch, sequence_length)` where padding elements are indicated by 0.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            use_cache (`bool`, *optional*):
                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
                (see `past_key_values`).
            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
                Indices depicting the position of the input sequence tokens in the sequence.
            kwargs (`dict`, *optional*):
                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code
                into the model
        """

        # Stocke l'entrée comme une connexion résiduelle
        # Store the input as a residual connection
        residual = hidden_states
        # Applique la normalisation de couche d'entrée
        # Apply input layer normalization
        hidden_states = self.input_layernorm(hidden_states)

        # Bloc d'auto-attention
        # Self-Attention block
        hidden_states, self_attn_weights, present_key_value = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
        )
        hidden_states = residual + hidden_states

        # Bloc entièrement connecté (propagation avant)
        # Fully Connected (Feed-Forward) block
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        # Ajoute la connexion résiduelle à la sortie de la propagation avant        
        # Add the residual connection to the feed-forward output
        hidden_states = residual + hidden_states
        
        # Prépare le tuple de sorties
        # Prepare the outputs tuple
        outputs = (hidden_states,)
        
        # Ajoute les poids d'attention aux sorties si demandé
        # Add attention weights to outputs if requested
        if output_attentions:
            outputs += (self_attn_weights,)
        # Ajoute les états clé/valeur présents aux sorties si la mise en cache est utilisée
        # Add present key/value states to outputs if caching is used
        if use_cache:
            outputs += (present_key_value,)

        return outputs

CHICKI_START_DOCSTRING = r"""
    Ce modèle hérite de [`PreTrainedModel`]. Consultez la documentation de la superclasse pour les méthodes génériques que la
    bibliothèque implémente pour tous ses modèles (comme le téléchargement ou l'enregistrement, le redimensionnement des incorporations d'entrée, l'élagage des têtes
    etc.)

    Ce modèle est également une sous-classe PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module).
    Utilisez-le comme un module PyTorch standard et reportez-vous à la documentation PyTorch pour tout ce qui concerne l'utilisation
    et le comportement général.

    Paramètres:
        config ([`ChickiConfig`]):
            Classe de configuration du modèle avec tous les paramètres du modèle. L'initialisation avec un fichier de configuration ne
            charge pas les poids associés au modèle, uniquement la configuration. Consultez la
            méthode [`~PreTrainedModel.from_pretrained`] pour charger les poids du modèle.

    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
    etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Parameters:
        config ([`ChickiConfig`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
"""

@add_start_docstrings(
    """
    Le modèle Chicki nu produisant des états cachés bruts sans aucune tête spécifique sur le dessus.
    
    The bare Chicki Model outputting raw hidden-states without any specific head on top.
    """,
    CHICKI_START_DOCSTRING,
)
class ChickiPreTrainedModel(PreTrainedModel):
    # Classe de base pour les modèles pré-entraînés Chicki
    # Base class for Chicki pre-trained models
    config_class = ChickiConfig
    # Classe de configuration associée au modèle
    # Associated configuration class for the model
    base_model_prefix = "model"
    # Préfixe utilisé pour le modèle de base
    # Prefix used for the base model
    supports_gradient_checkpointing = True
    # Indique que le modèle supporte le checkpointing de gradient pour l'optimisation de la mémoire
    # Indicates that the model supports gradient checkpointing for memory optimization
    _no_split_modules = ["ChickiDecoderLayer"]
    # Modules qui ne doivent pas être divisés lors du chargement du modèle
    # Modules that should not be split when loading the model
    _skip_keys_device_placement = "past_key_values"
    # Clés à ignorer lors du placement des tenseurs sur les dispositifs
    # Keys to skip when placing tensors on devices
    _supports_flash_attn_2 = True
    # Indique que le modèle supporte Flash Attention 2 pour une attention plus rapide
    # Indicates that the model supports Flash Attention 2 for faster attention computation
    _supports_sdpa = True
    # Indique que le modèle supporte l'attention à produit scalaire structuré
    # Indicates that the model supports structured dot product attention
    _supports_cache_class = True
    # Indique que le modèle supporte la classe de cache pour une génération plus efficace
    # Indicates that the model supports the cache class for more efficient generation

    def _init_weights(self, module):
        # Méthode pour initialiser les poids du modèle
        # Method to initialize the model weights
        std = self.config.initializer_range
        # Écart-type pour l'initialisation des poids
        # Standard deviation for weight initialization
        if isinstance(module, nn.Linear):
            # Initialisation des couches linéaires
            # Initialization of linear layers
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            # Initialisation des couches d'embedding
            # Initialization of embedding layers
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()

CHICKI_INPUTS_DOCSTRING = r"""
    Args:
        input_ids (`torch.LongTensor` de forme `(batch_size, sequence_length)`):
            Indices des tokens de la séquence d'entrée dans le vocabulaire. Le rembourrage sera ignoré par défaut si vous le fournissez.

            Les indices peuvent être obtenus en utilisant [`AutoTokenizer`]. Voir [`PreTrainedTokenizer.encode`] et
            [`PreTrainedTokenizer.__call__`] pour plus de détails.

            [Que sont les input IDs ?](../glossary#input-ids)
        attention_mask (`torch.Tensor` de forme `(batch_size, sequence_length)`, *optionnel*):
            Masque pour éviter d'effectuer l'attention sur les indices des tokens de rembourrage. Les valeurs du masque sont sélectionnées dans `[0, 1]` :

            - 1 pour les tokens qui **ne sont pas masqués**,
            - 0 pour les tokens qui **sont masqués**.

            [Que sont les masques d'attention ?](../glossary#attention-mask)

            Les indices peuvent être obtenus en utilisant [`AutoTokenizer`]. Voir [`PreTrainedTokenizer.encode`] et
            [`PreTrainedTokenizer.__call__`] pour plus de détails.

            Si `past_key_values` est utilisé, optionnellement seuls les derniers `input_ids` doivent être entrés (voir
            `past_key_values`).

            Si vous souhaitez modifier le comportement de rembourrage, vous devriez lire [`modeling_opt._prepare_decoder_attention_mask`]
            et le modifier selon vos besoins. Voir le diagramme 1 dans [l'article](https://arxiv.org/abs/1910.13461) pour plus
            d'informations sur la stratégie par défaut.
            
            - 1 indique que la tête **n'est pas masquée**,
            - 0 indique que la tête **est masquée**.
        position_ids (`torch.LongTensor` de forme `(batch_size, sequence_length)`, *optionnel*):
            Indices des positions de chaque token de la séquence d'entrée dans les embeddings de position. Sélectionnés dans la plage `[0,
            config.n_positions - 1]`.

            [Que sont les position IDs ?](../glossary#position-ids)
        past_key_values (`Cache` ou `tuple(tuple(torch.FloatTensor))`, *optionnel*):
            États cachés pré-calculés (clés et valeurs dans les blocs d'auto-attention et dans les blocs
            d'attention croisée) qui peuvent être utilisés pour accélérer le décodage séquentiel. Cela consiste généralement en les `past_key_values`
            retournés par le modèle à une étape précédente du décodage, lorsque `use_cache=True` ou `config.use_cache=True`.

            Deux formats sont autorisés :
            - une instance de [`~cache_utils.Cache`] ;
            - Tuple de `tuple(torch.FloatTensor)` de longueur `config.n_layers`, chaque tuple ayant 2 tenseurs de
            forme `(batch_size, num_heads, sequence_length, embed_size_per_head)`). C'est aussi connu comme le format
            de cache hérité.

            Le modèle produira le même format de cache que celui qui est fourni en entrée. Si aucun `past_key_values` n'est passé, le
            format de cache hérité sera retourné.

            Si `past_key_values` est utilisé, l'utilisateur peut optionnellement n'entrer que les derniers `input_ids` (ceux qui n'ont
            pas leurs états de valeurs de clés passées donnés à ce modèle) de forme `(batch_size, 1)` au lieu de tous les `input_ids`
            de forme `(batch_size, sequence_length)`.
        inputs_embeds (`torch.FloatTensor` de forme `(batch_size, sequence_length, hidden_size)`, *optionnel*):
            Optionnellement, au lieu de passer `input_ids`, vous pouvez choisir de passer directement une représentation embarquée. Ceci
            est utile si vous voulez plus de contrôle sur la façon de convertir les indices `input_ids` en vecteurs associés que la
            matrice de recherche d'embedding interne du modèle.
        use_cache (`bool`, *optionnel*):
            Si défini à `True`, les états de valeurs de clés `past_key_values` sont retournés et peuvent être utilisés pour accélérer le décodage (voir
            `past_key_values`).
        output_attentions (`bool`, *optionnel*):
            Si oui ou non retourner les tenseurs d'attention de toutes les couches d'attention. Voir `attentions` sous les tenseurs
            retournés pour plus de détails.
        output_hidden_states (`bool`, *optionnel*):
            Si oui ou non retourner les états cachés de toutes les couches. Voir `hidden_states` sous les tenseurs retournés pour
            plus de détails.
        return_dict (`bool`, *optionnel*):
            Si oui ou non retourner un [`~utils.ModelOutput`] au lieu d'un simple tuple.

    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
            it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
            `past_key_values`).

            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
            information on the default strategy.

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.
        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
            config.n_positions - 1]`.

            [What are position IDs?](../glossary#position-ids)
        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.

            Two formats are allowed:
            - a [`~cache_utils.Cache`] instance;
            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
            cache format.

            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
            legacy cache format will be returned.

            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
            of shape `(batch_size, sequence_length)`.
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
            model's internal embedding lookup matrix.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
            `past_key_values`).
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
            tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
            more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
            the complete sequence length.
"""

@add_start_docstrings(
    """
    Le modèle Chicki nu produisant des états cachés bruts sans aucune tête spécifique sur le dessus.
    
    The bare Chicki Model outputting raw hidden-states without any specific head on top.
    """,
    CHICKI_START_DOCSTRING,
)
class ChickiModel(ChickiPreTrainedModel):
    """
    Décodeur de transformateur composé de couches *config.num_hidden_layers*. Chaque couche est une [`ChickiDecoderLayer`]

    Args:
        config: ChickiConfig

    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`ChickiDecoderLayer`]

    Args:
        config: ChickiConfig
    """

    def __init__(self, config: ChickiConfig):
        super().__init__(config)
        # Définition de l'indice de padding et de la taille du vocabulaire
        # Definition of padding index and vocabulary size    
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size
        # Création de la couche d'embedding pour les tokens
        # Creation of the token embedding layer
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        # Création des couches de décodeur Chicki
        # Creation of Chicki decoder layers    
        self.layers = nn.ModuleList(
            [ChickiDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        
        # Définition de l'implémentation de l'attention
        # Definition of the attention implementation    
        self._attn_implementation = config._attn_implementation
        # Création de la couche de normalisation finale
        # Creation of the final normalization layer    
        self.norm = ChickiRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
    
        # Initialisation du gradient checkpointing
        # Initialization of gradient checkpointing
        self.gradient_checkpointing = False
        # Initialisation des poids et traitement final
        # Weight initialization and final processing    
        self.post_init()

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value

# Fonction de propagation avant du modèle Chicki
# Forward function of the Chicki model
    @add_start_docstrings_to_model_forward(CHICKI_INPUTS_DOCSTRING)
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Union[Tuple, BaseModelOutputWithPast]:
        # Configuration des paramètres de sortie
        # Configure output parameters    
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # Vérification de la cohérence des entrées
        # Check input consistency    
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError(
                "You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one"
            )

        # Gestion du gradient checkpointing
        # Handle gradient checkpointing
        if self.gradient_checkpointing and self.training:
            if use_cache:
                logger.warning_once(
                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                )
                use_cache = False

        # Gestion du cache hérité
        # Handle legacy cache
        use_legacy_cache = False
        if use_cache and not isinstance(past_key_values, Cache) and not self.training:
            use_legacy_cache = True
            past_key_values = DynamicCache.from_legacy_cache(past_key_values)
            logger.warning_once(
                "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. "
                "Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)"
            )

        # Génération des embeddings d'entrée
        # Generate input embeddings    
        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        # Gestion de la position du cache
        # Handle cache position    
        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )
        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        # Mise à jour du masque causal
        # Update causal mask    
        causal_mask = self._update_causal_mask(
            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions
        )

        hidden_states = inputs_embeds
        
        # Initialisation des variables pour les états cachés et l'attention
        # Initialize variables for hidden states and attention
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        next_decoder_cache = None

        # Boucle à travers les couches du décodeur
        # Loop through decoder layers    
        for decoder_layer in self.layers:
            if output_hidden_states:
                all_hidden_states += (hidden_states,)
        
            # Application du gradient checkpointing si nécessaire
            # Apply gradient checkpointing if needed
            if self.gradient_checkpointing and self.training:
                layer_outputs = self._gradient_checkpointing_func(
                    decoder_layer.__call__,
                    hidden_states,
                    causal_mask,
                    position_ids,
                    past_key_values,
                    output_attentions,
                    use_cache,
                    cache_position,
                )
            else:
                layer_outputs = decoder_layer(
                    hidden_states,
                    attention_mask=causal_mask,
                    position_ids=position_ids,
                    past_key_value=past_key_values,
                    output_attentions=output_attentions,
                    use_cache=use_cache,
                    cache_position=cache_position,
                )
            hidden_states = layer_outputs[0]
            if use_cache:
                next_decoder_cache = layer_outputs[2 if output_attentions else 1]
            if output_attentions:
                all_self_attns += (layer_outputs[1],)
                
        # Normalisation finale des états cachés
        # Final normalization of hidden states
        hidden_states = self.norm(hidden_states)
        # Ajout des états cachés de la dernière couche du décodeur
        # Add hidden states from the last decoder layer
        if output_hidden_states:
            all_hidden_states += (hidden_states,)
            
        # Préparation du cache pour le retour
        # Prepare cache for return
        next_cache = None
        if use_cache:
            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache
        
        # Retour des résultats
        # Return results
        if not return_dict:
            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=next_cache,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )

    # Copié de transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask
    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask
    def _update_causal_mask(
        self,
        attention_mask: torch.Tensor,
        input_tensor: torch.Tensor,
        cache_position: torch.Tensor,
        past_key_values: Cache,
        output_attentions: bool,
    ):
        # Cette méthode met à jour le masque causal pour le mécanisme d'attention
        # This method updates the causal mask for the attention mechanism

        # Gestion du cas pour Flash Attention 2
        # Handle Flash Attention 2 case
        if self.config._attn_implementation == "flash_attention_2":
            if attention_mask is not None and 0.0 in attention_mask:
                return attention_mask
            return None

        # Gestion du cas pour SDPA (Attention à Produit Scalaire Normalisé)
        # Handle SDPA (Scaled Dot Product Attention) case
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        using_static_cache = isinstance(past_key_values, StaticCache)

        if self.config._attn_implementation == "sdpa" and not using_static_cache and not output_attentions:
            if AttentionMaskConverter._ignore_causal_mask_sdpa(
                attention_mask,
                inputs_embeds=input_tensor,
                past_key_values_length=past_seen_tokens,
                is_training=self.training,
            ):
                return None

        # Préparation des variables pour la création du masque causal
        # Prepare variables for causal mask creation
        dtype, device = input_tensor.dtype, input_tensor.device
        min_dtype = torch.finfo(dtype).min
        sequence_length = input_tensor.shape[1]
        if using_static_cache:
            target_length = past_key_values.get_max_length()
        else:
            target_length = (
                attention_mask.shape[-1]
                if isinstance(attention_mask, torch.Tensor)
                else past_seen_tokens + sequence_length + 1
            )

        # Création du masque causal
        # Create the causal mask
        causal_mask = _prepare_4d_causal_attention_mask_with_cache_position(
            attention_mask,
            sequence_length=sequence_length,
            target_length=target_length,
            dtype=dtype,
            device=device,
            min_dtype=min_dtype,
            cache_position=cache_position,
            batch_size=input_tensor.shape[0],
        )
        # Gestion du cas spécial pour SDPA sur les dispositifs CUDA
        # Handle special case for SDPA on CUDA devices
        if (
            self.config._attn_implementation == "sdpa"
            and attention_mask is not None
            and attention_mask.device.type == "cuda"
            and not output_attentions
        ):
            # Démasquage des tokens non attendus pour une attention efficace en mémoire
            # Unmask unattended tokens for memory-efficient attention
            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)

        return causal_mask

class ChickiForCausalLM(ChickiPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]

    def __init__(self, config):
        # Initialisation du modèle avec la configuration donnée
        # Initialize the model with the given configuration    
        super().__init__(config)
        self.model = ChickiModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        # Initialisation des poids et application du traitement final
        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        # Récupère les embeddings d'entrée du modèle
        # Retrieve the input embeddings of the model        
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        # Définit de nouveaux embeddings d'entrée pour le modèle
        # Set new input embeddings for the model        
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        # Récupère les embeddings de sortie (lm_head)
        # Retrieve the output embeddings (lm_head)        
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        # Définit de nouveaux embeddings de sortie
        # Set new output embeddings        
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        # Remplace le décodeur actuel par un nouveau
        # Replace the current decoder with a new one        
        self.model = decoder

    def get_decoder(self):
        # Récupère le décodeur actuel du modèle
        # Retrieve the current decoder of the model        
        return self.model

    # Fonction de propagation avant (forward) pour le modèle Chicki
    # Forward function for the Chicki model
    @add_start_docstrings_to_model_forward(CHICKI_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        r"""
        Args:
            étiquettes (`torch.LongTensor` de forme `(batch_size, sequence_length)`, *optionnel*) :
                Étiquettes pour le calcul de la perte de modélisation de langage masquée. Les indices doivent être soit dans `[0, ...,
                config.vocab_size]` soit -100 (voir la docstring `input_ids`). Les jetons avec des indices définis sur `-100` sont ignorés
                (masqués), la perte n'est calculée que pour les jetons avec des étiquettes dans `[0, ..., config.vocab_size]`.

        Retours:

        Exemple:

        ```python
        >>> from transformers import AutoTokenizer, ChickiForCausalLM

        >>> model = ChickiForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)
        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)

        >>> prompt = "Hé, es-tu conscient ? Peux-tu me parler ?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Hé, es-tu conscient ? Peux-tu me parler ?\nJe ne suis pas conscient, mais je peux te parler."
        ```
        
        Args:
            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

        Returns:

        Example:

        ```python
        >>> from transformers import AutoTokenizer, ChickiForCausalLM

        >>> model = ChickiForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)
        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)

        >>> prompt = "Hey, are you conscious? Can you talk to me?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
        ```"""

        # Définir les valeurs par défaut pour les paramètres de sortie
        # Set default values for output parameters
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # Appeler le modèle principal pour obtenir les sorties du décodeur
        # Call the main model to get decoder outputs
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )

        # Extraire les états cachés et calculer les logits
        # Extract hidden states and compute logits
        hidden_states = outputs[0]
        logits = self.lm_head(hidden_states)
        logits = logits.float()

        # Calculer la perte si des labels sont fournis
        # Calculate loss if labels are provided
        loss = None
        if labels is not None:
            # Décaler les logits et les labels pour la prédiction du token suivant
            # Shift logits and labels for next token prediction
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Aplatir les tokens pour le calcul de la perte
            # Flatten the tokens for loss calculation
            loss_fct = CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Activer le parallélisme du modèle
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)
            loss = loss_fct(shift_logits, shift_labels)

        # Retourner les résultats selon le format spécifié
        # Return results in the specified format
        if not return_dict:
            output = (logits,) + outputs[1:]
            return (loss,) + output if loss is not None else output

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

    # Copié de transformers.models.llama.modeling_llama.LlamaForCausalLM.prepare_inputs_for_generation
    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.prepare_inputs_for_generation
    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        attention_mask=None,
        inputs_embeds=None,
        cache_position=None,
        position_ids=None,
        use_cache=True,
        **kwargs,
    ):
        # Préparation des entrées pour la génération de texte
        # Preparation of inputs for text generation

        # Si nous avons un cache, nous devons ajuster les input_ids
        # If we have a cache, we need to adjust the input_ids
        if past_key_values is not None:
            if inputs_embeds is not None:
                # Cas particulier où les embeddings sont fournis directement
                # Special case where embeddings are provided directly    
                input_ids = input_ids[:, -cache_position.shape[0] :]
            elif input_ids.shape[1] != cache_position.shape[0]: 
                # Cas par défaut : ajustement des input_ids en fonction du cache
                # Default case: adjusting input_ids based on the cache        
                input_ids = input_ids[:, cache_position]

        # Création des position_ids si non fournis et si un masque d'attention est présent
        # Creation of position_ids if not provided and if an attention mask is present
        if attention_mask is not None and position_ids is None:
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask == 0, 1)
            if past_key_values:
                position_ids = position_ids[:, -input_ids.shape[1] :]

                # Clone pour éviter les problèmes avec torch.compile
                # Clone to avoid issues with torch.compile
                position_ids = position_ids.clone(memory_format=torch.contiguous_format)

        # Gestion des inputs_embeds pour la première étape de génération
        # Handling inputs_embeds for the first generation step
        if inputs_embeds is not None and cache_position[0] == 0:
            model_inputs = {"inputs_embeds": inputs_embeds, "input_ids": None}
        else:
            # Clone pour la même raison que position_ids
            # Clone for the same reason as position_ids
            model_inputs = {"input_ids": input_ids.clone(memory_format=torch.contiguous_format), "inputs_embeds": None}

        # Préparation du masque d'attention pour le cache statique
        # Preparation of attention mask for static cache
        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:
            # Configuration des paramètres pour le masque d'attention
            # Setting up parameters for the attention mask    
            if model_inputs["inputs_embeds"] is not None:
                batch_size, sequence_length, _ = model_inputs["inputs_embeds"].shape
                device = model_inputs["inputs_embeds"].device
            else:
                batch_size, sequence_length = model_inputs["input_ids"].shape
                device = model_inputs["input_ids"].device

            dtype = self.lm_head.weight.dtype
            min_dtype = torch.finfo(dtype).min

            # Création du masque d'attention causal 4D avec cache
            # Creation of 4D causal attention mask with cache
            attention_mask = _prepare_4d_causal_attention_mask_with_cache_position(
                attention_mask,
                sequence_length=sequence_length,
                target_length=past_key_values.get_max_length(),
                dtype=dtype,
                device=device,
                min_dtype=min_dtype,
                cache_position=cache_position,
                batch_size=batch_size,
            )

        # Mise à jour des entrées du modèle avec les valeurs calculées
        # Updating model inputs with calculated values
        model_inputs.update(
            {
                "position_ids": position_ids,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "use_cache": use_cache,
                "attention_mask": attention_mask,
            }
        )
        return model_inputs

@add_start_docstrings(
    """
    Le transformateur Chicki Model avec une tête de classification de séquence au sommet (couche linéaire).

    [`ChickiForSequenceClassification`] utilise le dernier jeton pour effectuer la classification, comme le font d'autres modèles causaux
    (par exemple GPT-2).

    Comme il effectue la classification sur le dernier jeton, il doit connaître la position du dernier jeton. Si un
    `pad_token_id` est défini dans la configuration, il trouve le dernier jeton qui n'est pas un jeton de remplissage dans chaque ligne. Si
    aucun `pad_token_id` n'est défini, il prend simplement la dernière valeur de chaque ligne du lot. Comme il ne peut pas deviner les
    jetons de remplissage lorsque `inputs_embeds` sont passés au lieu de `input_ids`, il fait de même (prend la dernière valeur de
    chaque ligne du lot).

    The Chicki Model transformer with a sequence classification head on top (linear layer).

    [`ChickiForSequenceClassification`] uses the last token in order to do the classification, as other causal models
    (e.g. GPT-2) do.

    Since it does classification on the last token, it requires to know the position of the last token. If a
    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If
    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in
    each row of the batch).
    """,
    CHICKI_START_DOCSTRING,
)
class ChickiForSequenceClassification(ChickiPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.model = ChickiModel(config)
        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)
    
        # Initialisation des poids et traitement final
        # Initialize weights and apply final processing
        self.post_init()

    # Récupère les embeddings d'entrée du modèle
    # Get the input embeddings of the model
    def get_input_embeddings(self):
        return self.model.embed_tokens
    
    # Définit les embeddings d'entrée du modèle
    # Set the input embeddings of the model
    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    @add_start_docstrings_to_model_forward(CHICKI_INPUTS_DOCSTRING)
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        transformer_outputs = self.model(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        hidden_states = transformer_outputs[0]
        # Calcul des logits pour la classification
        # Calculation of logits for classification
        logits = self.score(hidden_states)

        # Détermination de la taille du batch
        # Determining the batch size
        if input_ids is not None:
            batch_size = input_ids.shape[0]
        else:
            batch_size = inputs_embeds.shape[0]

        # Vérification de la présence d'un token de padding pour les batchs > 1
        # Checking for the presence of a padding token for batches > 1
        if self.config.pad_token_id is None and batch_size != 1:
            raise ValueError("Cannot handle batch sizes > 1 if no padding token is defined.")
        
        # Calcul des longueurs de séquence
        # Calculation of sequence lengths
        if self.config.pad_token_id is None:
            sequence_lengths = -1
        else:
            if input_ids is not None:
                # Utilisation de l'argmax pour la compatibilité ONNX
                # Using argmax for ONNX compatibility    
                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1
                sequence_lengths = sequence_lengths % input_ids.shape[-1]
                sequence_lengths = sequence_lengths.to(logits.device)
            else:
                sequence_lengths = -1

        # Extraction des logits poolés pour chaque séquence
        # Extraction of pooled logits for each sequence
        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]

        # Initialisation de la variable de perte
        # Initialization of the loss variable
        loss = None
        # Calcul de la perte si des labels sont fournis
        # Calculation of loss if labels are provided
        if labels is not None:
            labels = labels.to(logits.device)
            # Détermination automatique du type de problème si non spécifié
            # Automatic determination of problem type if not specified    
            if self.config.problem_type is None:
                if self.num_labels == 1:
                    self.config.problem_type = "regression"
                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"

            # Calcul de la perte en fonction du type de problème
            # Calculation of loss based on problem type    
            if self.config.problem_type == "regression":
                loss_fct = MSELoss()
                if self.num_labels == 1:
                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())
                else:
                    loss = loss_fct(pooled_logits, labels)
            elif self.config.problem_type == "single_label_classification":
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = BCEWithLogitsLoss()
                loss = loss_fct(pooled_logits, labels)
        # Préparation de la sortie en fonction du paramètre return_dict
        # Preparation of output based on return_dict parameter        
        if not return_dict:
            output = (pooled_logits,) + transformer_outputs[1:]
            return ((loss,) + output) if loss is not None else output

        return SequenceClassifierOutputWithPast(
            loss=loss,
            logits=pooled_logits,
            past_key_values=transformer_outputs.past_key_values,
            hidden_states=transformer_outputs.hidden_states,
            attentions=transformer_outputs.attentions,
        )

@add_start_docstrings(
    """
    Le transformateur du modèle Chicki avec une tête de classification de jetons au-dessus (une couche linéaire au-dessus 
    de la sortie des états cachés) par exemple pour les tâches de reconnaissance d'entités nommées (NER).

    The Chicki Model transformer with a token classification head on top (a linear layer on top of the hidden-states
    output) e.g. for Named-Entity-Recognition (NER) tasks.
    """,
    CHICKI_START_DOCSTRING,
)
# Copié de transformers.models.llama.modeling_llama.LlamaForTokenClassification avec Llama->Chicki, LLAMA->CHICKI
# Copied from transformers.models.llama.modeling_llama.LlamaForTokenClassification with Llama->Chicki, LLAMA->CHICKI
class ChickiForTokenClassification(ChickiPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        # Initialise le nombre d'étiquettes à partir de la configuration
        # Initialize the number of labels from the configuration        
        self.num_labels = config.num_labels
        # Crée une instance du modèle Chicki
        # Create an instance of the Chicki model        
        self.model = ChickiModel(config)
        
        # Détermine la valeur du dropout pour le classificateur
        # Determine the dropout value for the classifier        
        if getattr(config, "classifier_dropout", None) is not None:
            classifier_dropout = config.classifier_dropout
        elif getattr(config, "hidden_dropout", None) is not None:
            classifier_dropout = config.hidden_dropout
        else:
            classifier_dropout = 0.1
            
        # Crée une couche de dropout pour le classificateur
        # Create a dropout layer for the classifier            
        self.dropout = nn.Dropout(classifier_dropout)
        # Crée une couche linéaire pour le score final
        # Create a linear layer for the final score        
        self.score = nn.Linear(config.hidden_size, config.num_labels)
        # Initialise les poids et applique le traitement final
        # Initialize weights and apply final processing
        self.post_init()

    # Méthode pour obtenir les embeddings d'entrée
    # Method to get the input embeddings
    def get_input_embeddings(self):
        return self.model.embed_tokens

    # Méthode pour définir les embeddings d'entrée
    # Method to set the input embeddings
    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    @add_start_docstrings_to_model_forward(CHICKI_INPUTS_DOCSTRING)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, TokenClassifierOutput]:
        r"""
        labels (`torch.LongTensor` de forme `(batch_size,)`, *optional*):
            Libellés pour le calcul de la perte de classification/régression de séquence. Les indices doivent être dans `[0, ...,
            config.num_labels - 1]`. Si `config.num_labels == 1` une perte de régression est calculée (perte quadratique moyenne), si
            `config.num_labels > 1` une perte de classification est calculée (entropie croisée).

        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """
        # Définir la valeur de return_dict si non spécifiée
        # Set the value of return_dict if not specified    
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # Appeler le modèle principal
        # Call the main model
        outputs = self.model(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        # Extraire la sortie de séquence et appliquer le dropout
        # Extract the sequence output and apply dropout    
        sequence_output = outputs[0]
        sequence_output = self.dropout(sequence_output)
        # Calculer les logits
        # Calculate the logits    
        logits = self.score(sequence_output)

        # Calculer la perte si des labels sont fournis
        # Calculate the loss if labels are provided
        loss = None
        if labels is not None:
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

        # Retourner les résultats sous forme de tuple si return_dict est False
        # Return the results as a tuple if return_dict is False
        if not return_dict:
            output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return TokenClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

