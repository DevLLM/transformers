# coding=utf-8
## Licence d'Utilisation Chicki
#
#Copyright (c) 2024 Chicki. Tous droits réservés.
#
#Cette licence régit l'utilisation du modèle Chicki. En utilisant, copiant ou distribuant ce modèle, vous acceptez de vous conformer aux termes et conditions suivants.
#
### 1. Politique d'Utilisation Acceptable
#
#Vous devez respecter la Politique d'Utilisation Acceptable de Chicki comme détaillée ci-dessous. Cela inclut des restrictions sur les activités illégales, les comportements nuisibles, et l'utilisation abusive du modèle.
#
#### 1.1 Usages Interdits
#Vous vous engagez à ne pas utiliser le modèle, ni à permettre à d'autres de l'utiliser, pour toute finalité qui :
#
#1. Violerait une loi ou porterait atteinte aux droits d'autrui, y compris :
#   - Participer à ou promouvoir des activités illégales telles que la violence, le terrorisme, la traite des êtres humains, l'exploitation de mineurs, ou tout autre acte illicite.
#   - Faciliter le harcèlement, les abus, la discrimination, ou l'exercice non autorisé de toute profession.
#   - Collecter ou traiter des données personnelles sensibles sans autorisation appropriée.
#   - Créer ou distribuer des logiciels malveillants ou perturber autrement les systèmes numériques.
#
#2. Impliquerait des activités présentant un risque de préjudice physique ou de mort pour des individus, telles que :
#   - Applications liées à des activités militaires, nucléaires, ou d'espionnage.
#   - Développement ou utilisation d'armes illégales, de drogues ou de substances contrôlées.
#   - Exploitation d'infrastructures critiques, de technologies de transport ou de machines lourdes.
#   - Promouvoir l'automutilation, le suicide ou toute autre forme de préjudice corporel.
#
#3. Tromperait ou induirait intentionnellement en erreur les autres, y compris :
#   - Générer ou promouvoir du contenu frauduleux, de la désinformation, ou des déclarations diffamatoires.
#   - Distribuer du spam ou se faire passer pour autrui sans consentement.
#   - Représenter à tort que les sorties du modèle sont générées par des humains.
#   - Faciliter un engagement en ligne faux, comme de fausses critiques.
#
#4. Ne divulgue pas de manière adéquate aux utilisateurs finaux les risques connus associés à votre système d'IA.
#
### 2. Signalement des Violations
#
#Les violations de cette Politique, ainsi que les bugs logiciels ou les problèmes de sécurité, doivent être signalés via les canaux suivants :
#- Signaler des problèmes avec le modèle : [https://github.com/ChickiLM](https://github.com/ChickiLM)
#- Signaler des bugs et des problèmes de sécurité : Chicki-Bugs@llm.ci
#- Signaler des violations de la Politique d'Utilisation Acceptable : Chicki-Viola@llm.ci
#
### 3. Avertissement
#
#LE MODÈLE EST FOURNI "TEL QUEL", SANS AUCUNE GARANTIE, EXPRESSE OU IMPLICITE, Y COMPRIS MAIS SANS S'Y LIMITER AUX GARANTIES DE QUALITÉ MARCHANDE, D'ADÉQUATION À UN USAGE PARTICULIER, ET DE NON-VIOLATION. EN AUCUN CAS LE DÉTENTEUR DU DROIT D'AUTEUR OU LES CONTRIBUTEURS NE SERONT RESPONSABLES DE TOUTE RÉCLAMATION, DOMMAGE OU AUTRE RESPONSABILITÉ, QUE CE SOIT DANS LE CADRE D'UN CONTRAT, D'UN DÉLIT OU AUTREMENT, DÉCOULANT DE, DEPUIS OU EN LIEN AVEC LE MODÈLE OU L'UTILISATION OU AUTRES INTERACTIONS AVEC LE MODÈLE.
#
### 4. Droit Applicable
#
#Cette licence est régie et interprétée conformément aux lois françaises, à l'exclusion de ses dispositions relatives aux conflits de lois.
#
### 5. Résiliation
#
#Cette licence est effective jusqu'à sa résiliation. Vos droits en vertu de cette licence seront automatiquement résiliés sans préavis si vous ne respectez pas les termes. À la résiliation, vous devez cesser toute utilisation et détruire toutes les copies du modèle.
#
### 6. Informations de Contact
#
#Pour toute question ou problème lié à cette licence, veuillez contacter Chicki à Chicki-Contact@llm.ci.
#
#
#
## Chicki License Agreement
#
#Copyright (c) 2024 Chicki. All rights reserved.
#
#This license governs the use of the Chicki model. By using, copying, or distributing this model, you agree to comply with the following terms and conditions.
#
### 1. Acceptable Use Policy
#
#You must comply with the Chicki Acceptable Use Policy as detailed below. This includes restrictions on illegal activities, harmful behavior, and misuse of the model.
#
#### 1.1 Prohibited Uses
#You agree not to use the model, nor allow others to use it, for any purpose that:
#
#1. Violates any law or infringes the rights of others, including:
#   - Engaging in or promoting illegal activities such as violence, terrorism, human trafficking, exploitation of minors, or any other unlawful act.
#   - Facilitating harassment, abuse, discrimination, or unauthorized practice of any profession.
#   - Collecting or processing sensitive personal data without proper authorization.
#   - Creating or distributing malicious software or otherwise disrupting digital systems.
#
#2. Involves activities that pose a risk of physical harm or death to individuals, such as:
#   - Applications related to military, nuclear, or espionage activities.
#   - Development or use of illegal weapons, drugs, or controlled substances.
#   - Exploitation of critical infrastructure, transportation technologies, or heavy machinery.
#   - Promoting self-harm, suicide, or other forms of bodily harm.
#
#3. Intentionally deceives or misleads others, including:
#   - Generating or promoting fraudulent content, misinformation, or defamatory statements.
#   - Distributing spam or impersonating others without consent.
#   - Misrepresenting the model's outputs as being generated by humans.
#   - Facilitating false online engagement, such as fake reviews.
#
#4. Fails to adequately inform end-users about known risks associated with your AI system.
#
### 2. Reporting Violations
#
#Violations of this Policy, as well as software bugs or security issues, should be reported through the following channels:
#- Report issues with the model: [https://github.com/ChickiLM](https://github.com/ChickiLM)
#- Report bugs and security issues: Chicki-Bugs@llm.ci
#- Report Acceptable Use Policy violations: Chicki-Viola@llm.ci
#
### 3. Disclaimer
#
#THE MODEL IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT, OR OTHERWISE, ARISING FROM, OUT OF, OR IN CONNECTION WITH THE MODEL OR THE USE OR OTHER DEALINGS IN THE MODEL.
#
### 4. Governing Law
#
#This license shall be governed by and construed in accordance with the laws of French, excluding its conflict of law provisions.
#
### 5. Termination
#
#This license is effective until terminated. Your rights under this license will terminate automatically without notice if you fail to comply with the terms. Upon termination, you must cease all use and destroy all copies of the model.
#
### 6. Contact Information
#
#For any inquiries or issues related to this license, please contact Chicki at Chicki-Contact@llm.ci
"""Classes de tokenisation pour Chicki."""
"""Tokenization classes for Chicki."""

import json
import os
import unicodedata
from functools import lru_cache
from typing import Optional, Tuple

import regex as re

from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer
from transformers.utils import logging

logger = logging.get_logger(__name__)

VOCAB_FILES_NAMES = {
    "vocab_file": "vocab.json",
    "merges_file": "merges.txt",
}

MAX_MODEL_INPUT_SIZES = {"ChickiLM/Chicki-tokenizer": 32768}

PRETOKENIZE_REGEX = r"""(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+"""

@lru_cache()
# Copié de transformers.models.gpt2.tokenization_gpt2.bytes_to_unicode
# Copied from transformers.models.gpt2.tokenization_gpt2.bytes_to_unicode
def bytes_to_unicode():
    """
    Renvoie une liste d'octets utf-8 et un mappage vers des chaînes unicode. Nous évitons spécifiquement le mappage vers des espaces/caractères de contrôle
    sur lesquels le code bpe se bloque.

    Les codes bpe réversibles fonctionnent sur des chaînes unicode. Cela signifie que vous avez besoin d'un grand nombre de caractères unicode dans votre vocabulaire
    si vous voulez éviter les UNK. Lorsque vous êtes sur un ensemble de données de jetons de l'ordre de 10 B, vous finissez par avoir besoin d'environ 5 000 pour une
    couverture décente. Il s'agit d'un pourcentage significatif de votre vocabulaire normal, disons, de 32 Ko bpe. Pour éviter cela, nous voulons des tables de recherche
    entre les octets utf-8 et les chaînes unicode.


    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control
    characters the bpe code barfs on.

    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab
    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for
    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup
    tables between utf-8 bytes and unicode strings.
    """
    # Initialiser la liste des octets avec les caractères ASCII imprimables
    # Initialize the list of bytes with printable ASCII characters    
    bs = (
        list(range(ord("!"), ord("~") + 1)) + 
        list(range(ord("¡"), ord("¬") + 1)) + 
        list(range(ord("®"), ord("ÿ") + 1))
    )
    cs = bs[:]  # Copie de la liste des octets / Copy of the byte list
    n = 0  # Compteur pour les nouveaux caractères / Counter for new characters
    
    # Ajouter les octets manquants et leur attribuer de nouveaux caractères unicode
    # Add missing bytes and assign them new unicode characters    
    for b in range(2**8):
        if b not in bs:
            bs.append(b)
            cs.append(2**8 + n)
            n += 1
    # Convertir les codes de caractères en caractères unicode réels
    # Convert character codes to actual unicode characters            
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))

# Copié de transformers.models.gpt2.tokenization_gpt2.get_pairs
# Copied from transformers.models.gpt2.tokenization_gpt2.get_pairs
def get_pairs(word):
    """
    Retourne l'ensemble des paires de symboles dans un mot.
    Return set of symbol pairs in a word.

    Le mot est représenté comme un tuple de symboles (les symboles étant des chaînes de longueur variable).
    Word is represented as tuple of symbols (symbols being variable-length strings).
    """
    # Initialise un ensemble vide pour stocker les paires
    # Initialize an empty set to store the pairs
    pairs = set()
    # Définit le premier caractère comme le caractère précédent
    # Set the first character as the previous character
    prev_char = word[0]
    # Itère à travers le mot en commençant par le deuxième caractère
    # Iterate through the word starting from the second character
    for char in word[1:]:
        # Ajoute la paire du caractère précédent et du caractère actuel à l'ensemble
        # Add the pair of previous and current character to the set
        pairs.add((prev_char, char))
        # Met à jour le caractère précédent pour la prochaine itération
        # Update the previous character for the next iteration
        prev_char = char
    # Retourne l'ensemble de toutes les paires trouvées dans le mot
    # Return the set of all pairs found in the word
    return pairs

class ChickiTokenizer(PreTrainedTokenizer):
    """
    Construisez un tokenizer Chicki. Basé sur le Byte-Pair-Encoding au niveau des octets.

    De même que GPT2Tokenizer, ce tokenizer a été formé pour traiter les espaces comme des parties des tokens, donc un mot sera
    encodé différemment qu'il soit au début de la phrase (sans espace) ou non:

    ```python
    >>> from transformers import ChickiTokenizer

    >>> tokenizer = ChickiTokenizer.from_pretrained("ChickiLM/Chicki-tokenizer")
    >>> tokenizer("Hello world")["input_ids"]
    [9707, 1879]

    >>> tokenizer("Hello world")["input_ids"]
    [21927, 1879]
    ```
    Ceci est attendu.

    Vous ne devez pas utiliser GPT2Tokenizer à la place, en raison des différentes règles de pré-tokenisation.

    Ce tokenizer hérite de [`PreTrainedTokenizer`] qui contient la plupart des méthodes principales. Les utilisateurs doivent se référer à
    cette superclasse pour plus d'informations concernant ces méthodes.

    Args:
        vocab_file (`str`):
            Chemin vers le fichier de vocabulaire.
        merges_file (`str`):
            Chemin vers le fichier de fusion.
        errors (`str`, *optional*, defaults to `"replace"`):
            Paradigme à suivre lors du décodage d'octets en UTF-8. Voir
            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) pour plus d'informations.
        unk_token (`str`, *optional*, defaults to `"<|endoftext|>"`):
            Le jeton inconnu. Un jeton qui n'est pas dans le vocabulaire ne peut pas être converti en ID et est défini comme ce
            jeton à la place.
        bos_token (`str`, *optional*):
            Le jeton de début de séquence. Non applicable pour ce tokenizer.
        eos_token (`str`, *optional*, defaults to `"<|endoftext|>"`):
            Le jeton de fin de séquence.
        pad_token (`str`, *optional*, defaults to `"<|endoftext|>"`):
            Le jeton utilisé pour le remplissage, par exemple lors du regroupement de séquences de longueurs différentes.
        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):
            Indique si le modèle doit ou non nettoyer les espaces qui ont été ajoutés lors du fractionnement du texte d'entrée pendant le
            processus de tokenisation. Non applicable à ce tokenizer, car la tokenisation n'ajoute pas d'espaces.
        split_special_tokens (`bool`, *optional*, defaults to `False`):
            Indique si les jetons spéciaux doivent ou non être fractionnés pendant le processus de tokenisation. Le comportement par défaut est
            de ne pas fractionner les jetons spéciaux. Cela signifie que si `<|endoftext|>` est le `eos_token`, alors `tokenizer.tokenize("<|endoftext|>") =
            ['<|endoftext|>`]. Sinon, si `split_special_tokens=True`, alors `tokenizer.tokenize("<|endoftext|>")` donnera `['<',
            '|', 'endo', 'ft', 'ext', '|', '>']`. Cet argument n'est pris en charge que pour les tokenizers `slow` pour le moment.


    Construct a Chicki tokenizer. Based on byte-level Byte-Pair-Encoding.

    Same with GPT2Tokenizer, this tokenizer has been trained to treat spaces like parts of the tokens so a word will
    be encoded differently whether it is at the beginning of the sentence (without space) or not:

    ```python
    >>> from transformers import ChickiTokenizer

    >>> tokenizer = ChickiTokenizer.from_pretrained("ChickiLM/Chicki-tokenizer")
    >>> tokenizer("Hello world")["input_ids"]
    [9707, 1879]

    >>> tokenizer(" Hello world")["input_ids"]
    [21927, 1879]
    ```
    This is expected.

    You should not use GPT2Tokenizer instead, because of the different pretokenization rules.

    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to
    this superclass for more information regarding those methods.

    Args:
        vocab_file (`str`):
            Path to the vocabulary file.
        merges_file (`str`):
            Path to the merges file.
        errors (`str`, *optional*, defaults to `"replace"`):
            Paradigm to follow when decoding bytes to UTF-8. See
            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.
        unk_token (`str`, *optional*, defaults to `"<|endoftext|>"`):
            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
            token instead.
        bos_token (`str`, *optional*):
            The beginning of sequence token. Not applicable for this tokenizer.
        eos_token (`str`, *optional*, defaults to `"<|endoftext|>"`):
            The end of sequence token.
        pad_token (`str`, *optional*, defaults to `"<|endoftext|>"`):
            The token used for padding, for example when batching sequences of different lengths.
        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):
            Whether or not the model should cleanup the spaces that were added when splitting the input text during the
            tokenization process. Not applicable to this tokenizer, since tokenization does not add spaces.
        split_special_tokens (`bool`, *optional*, defaults to `False`):
            Whether or not the special tokens should be split during the tokenization process. The default behavior is
            to not split special tokens. This means that if `<|endoftext|>` is the `eos_token`, then `tokenizer.tokenize("<|endoftext|>") =
            ['<|endoftext|>`]. Otherwise, if `split_special_tokens=True`, then `tokenizer.tokenize("<|endoftext|>")` will be give `['<',
            '|', 'endo', 'ft', 'ext', '|', '>']`. This argument is only supported for `slow` tokenizers for the moment.
    """

    vocab_files_names = VOCAB_FILES_NAMES
    model_input_names = ["input_ids", "attention_mask"]

    def __init__(
        self,
        vocab_file,
        merges_file,
        errors="replace",
        unk_token="<|endoftext|>",
        bos_token=None,
        eos_token="<|endoftext|>",
        pad_token="<|endoftext|>",
        clean_up_tokenization_spaces=False,
        split_special_tokens=False,
        **kwargs,
    ):
        # Nous devons au moins passer vocab_file et merges_file à la classe de base
        # au cas où un tokenizer lent doit être initialisé ; d'autres peuvent être
        # configurés via des fichiers.
        # après GPT2TokenizerFast, en ajoutant également unk_token, bos_token et eos_token
        
        # We need to at least pass vocab_file and merges_file to base class
        # in case a slow tokenizer needs to be initialized; other can be
        # configured through files.
        # following GPT2TokenizerFast, also adding unk_token, bos_token, and eos_token

        bos_token = (
            AddedToken(bos_token, lstrip=False, rstrip=False, special=True, normalized=False)
            if isinstance(bos_token, str)
            else bos_token
        )
        eos_token = (
            AddedToken(eos_token, lstrip=False, rstrip=False, special=True, normalized=False)
            if isinstance(eos_token, str)
            else eos_token
        )
        unk_token = (
            AddedToken(unk_token, lstrip=False, rstrip=False, special=True, normalized=False)
            if isinstance(unk_token, str)
            else unk_token
        )
        pad_token = (
            AddedToken(pad_token, lstrip=False, rstrip=False, special=True, normalized=False)
            if isinstance(pad_token, str)
            else pad_token
        )
        
        # Chargement du vocabulaire à partir du fichier
        # Loading vocabulary from file
        with open(vocab_file, encoding="utf-8") as vocab_handle:
            self.encoder = json.load(vocab_handle)
        self.decoder = {v: k for k, v in self.encoder.items()}
        self.errors = errors
        self.byte_encoder = bytes_to_unicode()
        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}
        # Chargement et traitement des fusions BPE
        # Loading and processing BPE merges    
        bpe_merges = []
        with open(merges_file, encoding="utf-8") as merges_handle:
            for i, line in enumerate(merges_handle):
                line = line.strip()
                if (i == 0 and line.startswith("#version:")) or not line:
                    continue
                bpe_merges.append(tuple(line.split()))
        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))

        # Initialisation du cache (attention à la croissance potentielle)
        # Initializing cache (beware of potential growth)
        self.cache = {}
        # Compilation de l'expression régulière pour le pré-tokenization
        # Compiling regular expression for pre-tokenization
        self.pat = re.compile(PRETOKENIZE_REGEX)

        # Avertissement pour l'option non supportée
        # English: Warning for unsupported option
        if kwargs.get("add_prefix_space", False):
            logger.warning_once(
                f"{self.__class__.__name} does not support `add_prefix_space`, setting it to True has no effect."
            )

        # Appel du constructeur de la classe parente avec les paramètres configurés
        # Calling parent class constructor with configured parameters
        super().__init__(
            errors=errors,
            bos_token=bos_token,
            eos_token=eos_token,
            pad_token=pad_token,
            unk_token=unk_token,
            clean_up_tokenization_spaces=clean_up_tokenization_spaces,
            split_special_tokens=split_special_tokens,
            **kwargs,
        )

    @property
    def vocab_size(self) -> int:
        return len(self.encoder)

    # Copié de transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.get_vocab
    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.get_vocab
    def get_vocab(self):
        return dict(self.encoder, **self.added_tokens_encoder)

    # Copié de transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.bpe
    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.bpe
    def bpe(self, token):
        # Vérifie si le token est déjà dans le cache et le retourne s'il y est
        # Check if the token is already in the cache and return it if it is    
        if token in self.cache:
            return self.cache[token]
        # Convertit le token en tuple de caractères
        # Convert the token to a tuple of characters        
        word = tuple(token)
        # Obtient toutes les paires de caractères adjacents dans le mot
        # Get all adjacent character pairs in the word        
        pairs = get_pairs(word)

        # Si aucune paire n'est trouvée, retourne le token original
        # If no pairs are found, return the original token
        if not pairs:
            return token

        while True:
            # Trouve la paire avec le rang BPE le plus bas
            # Find the pair with the lowest BPE rank            
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))
            # Si la paire n'est pas dans les rangs BPE, sortir de la boucle
            # If the pair is not in the BPE ranks, exit the loop            
            if bigram not in self.bpe_ranks:
                break
            # Décompose la paire en premier et second éléments
            # Decompose the pair into first and second elements            
            first, second = bigram
            new_word = []
            i = 0
            
            # Parcourt le mot pour appliquer la fusion BPE
            # Iterate through the word to apply BPE merging            
            while i < len(word):
                try:
                    # Trouve l'index du premier élément de la paire
                    # Find the index of the first element of the pair                    
                    j = word.index(first, i)
                    new_word.extend(word[i:j])
                    i = j
                except ValueError:
                    # Si le premier élément n'est pas trouvé, ajoute le reste du mot
                    # If the first element is not found, add the rest of the word                    
                    new_word.extend(word[i:])
                    break

                # Si la paire est trouvée, fusionne-la
                # If the pair is found, merge it
                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:
                    new_word.append(first + second)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
                    
            # Met à jour le mot et les paires pour la prochaine itération
            # Update the word and pairs for the next iteration                    
            new_word = tuple(new_word)
            word = new_word
            if len(word) == 1:
                break
            else:
                pairs = get_pairs(word)
        # Convertit le mot final en chaîne de caractères
        # Convert the final word to a string                
        word = " ".join(word)
        # Stocke le résultat dans le cache et le retourne
        # Store the result in the cache and return it        
        self.cache[token] = word
        return word

    # Copié de transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._tokenize
    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._tokenize
    def _tokenize(self, text):
        # Tokenise une chaîne de caractères en utilisant l'encodage BPE (Byte Pair Encoding)
        # Tokenize a string using Byte Pair Encoding (BPE)
        bpe_tokens = []
        for token in re.findall(self.pat, text):
            # Pour chaque token trouvé dans le texte en utilisant l'expression régulière self.pat
            # For each token found in the text using the regular expression self.pat    
            token = "".join(
                self.byte_encoder[b] for b in token.encode("utf-8")
            )
            # Encode chaque byte du token en utilisant byte_encoder
            # Encode each byte of the token using byte_encoder
            
            # Applique l'algorithme BPE au token encodé et ajoute les sous-tokens résultants à la liste
            # Apply the BPE algorithm to the encoded token and add the resulting sub-tokens to the list
            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(" "))
        return bpe_tokens

    # Copié de transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._convert_token_to_id
    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._convert_token_to_id
    def _convert_token_to_id(self, token):
        # Convertit un token (chaîne de caractères) en un identifiant en utilisant le vocabulaire
        # Convert a token (string) to an ID using the vocabulary
        return self.encoder.get(token, self.encoder.get(self.unk_token))

    # Copié de transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._convert_id_to_token
    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._convert_id_to_token
    def _convert_id_to_token(self, index):
        # Convertit un index (entier) en un token (chaîne de caractères) en utilisant le vocabulaire
        # Convert an index (integer) to a token (string) using the vocabulary
        return self.decoder.get(index)

    # Copié de transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.convert_tokens_to_string
    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.convert_tokens_to_string
    def convert_tokens_to_string(self, tokens):
        # Concaténer tous les tokens en une seule chaîne
        # Concatenate all tokens into a single string
        text = "".join(tokens)
        # Decode the text using the byte decoder and UTF-8 encoding
        # Décoder le texte en utilisant le décodeur d'octets et l'encodage UTF-8        
        text = bytearray([self.byte_decoder[c] for c in text]).decode("utf-8", errors=self.errors)
        return text

    def decode(
        self,
        token_ids,
        skip_special_tokens: bool = False,
        clean_up_tokenization_spaces: Optional[bool] = False,
        spaces_between_special_tokens: bool = False,
        **kwargs,
    ) -> str:
        """
        Décode une séquence d'IDs de tokens en une chaîne de caractères.
        Decode a sequence of token IDs back to a string.

        Paramètres:
        - token_ids: La séquence d'IDs de tokens à décoder
        - skip_special_tokens: Indique s'il faut supprimer les tokens spéciaux de la sortie
        - clean_up_tokenization_spaces: Indique s'il faut nettoyer les espaces de tokenization (non utilisé dans cette implémentation)
        - spaces_between_special_tokens: Indique s'il faut ajouter des espaces entre les tokens spéciaux (défini à False pour ChickiTokenizer)

        Parameters:
        - token_ids: The sequence of token IDs to decode
        - skip_special_tokens: Whether to remove special tokens from the output
        - clean_up_tokenization_spaces: Whether to clean up tokenization spaces (not used in this implementation)
        - spaces_between_special_tokens: Whether to add spaces between special tokens (set to False for ChickiTokenizer)
        """
        # Appeler la méthode decode de la classe parente avec les paramètres spécifiés
        # Call the parent class's decode method with the specified parameters
        return super().decode(
            token_ids,
            skip_special_tokens=skip_special_tokens,
            clean_up_tokenization_spaces=clean_up_tokenization_spaces,
            spaces_between_special_tokens=spaces_between_special_tokens,
            **kwargs,
        )

    # Copié de transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.save_vocabulary
    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.save_vocabulary
    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:
        # Vérifie si le répertoire de sauvegarde existe
        # Check if the save directory exists        
        if not os.path.isdir(save_directory):
            logger.error(f"Vocabulary path ({save_directory}) should be a directory")
            return
        
        # Construit les chemins pour les fichiers de vocabulaire et de fusion
        # Construct paths for vocabulary and merge files        
        vocab_file = os.path.join(
            save_directory, (filename_prefix + "-" if filename_prefix else "") + VOCAB_FILES_NAMES["vocab_file"]
        )
        merge_file = os.path.join(
            save_directory, (filename_prefix + "-" if filename_prefix else "") + VOCAB_FILES_NAMES["merges_file"]
        )

        # Sauvegarde le vocabulaire dans un fichier JSON
        # Save the vocabulary to a JSON file
        with open(vocab_file, "w", encoding="utf-8") as f:
            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + "\n")

        # Sauvegarde les fusions BPE dans un fichier texte
        # Save the BPE merges to a text file
        index = 0
        with open(merge_file, "w", encoding="utf-8") as writer:
            writer.write("#version: 0.2\n")
            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):
                # Vérifie si les indices sont consécutifs
                # Check if the indices are consecutive                
                if index != token_index:
                    logger.warning(
                        f"Saving vocabulary to {merge_file}: BPE merge indices are not consecutive."
                        " Please check that the tokenizer is not corrupted!"
                    )
                    index = token_index
                writer.write(" ".join(bpe_tokens) + "\n")
                index += 1

        # Retourne les chemins des fichiers sauvegardés
        # Return the paths of the saved files
        return vocab_file, merge_file

    def prepare_for_tokenization(self, text, **kwargs):
        # Normalise le texte en forme de composition canonique (NFC)
        # Normalize the text to canonical composition form (NFC)        
        text = unicodedata.normalize("NFC", text)
        return (text, kwargs)

