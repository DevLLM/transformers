# coding=utf-8
## Licence d'Utilisation Chicki
#
#Copyright (c) 2024 Chicki. Tous droits réservés.
#
#Cette licence régit l'utilisation du modèle Chicki. En utilisant, copiant ou distribuant ce modèle, vous acceptez de vous conformer aux termes et conditions suivants.
#
### 1. Politique d'Utilisation Acceptable
#
#Vous devez respecter la Politique d'Utilisation Acceptable de Chicki comme détaillée ci-dessous. Cela inclut des restrictions sur les activités illégales, les comportements nuisibles, et l'utilisation abusive du modèle.
#
#### 1.1 Usages Interdits
#Vous vous engagez à ne pas utiliser le modèle, ni à permettre à d'autres de l'utiliser, pour toute finalité qui :
#
#1. Violerait une loi ou porterait atteinte aux droits d'autrui, y compris :
#   - Participer à ou promouvoir des activités illégales telles que la violence, le terrorisme, la traite des êtres humains, l'exploitation de mineurs, ou tout autre acte illicite.
#   - Faciliter le harcèlement, les abus, la discrimination, ou l'exercice non autorisé de toute profession.
#   - Collecter ou traiter des données personnelles sensibles sans autorisation appropriée.
#   - Créer ou distribuer des logiciels malveillants ou perturber autrement les systèmes numériques.
#
#2. Impliquerait des activités présentant un risque de préjudice physique ou de mort pour des individus, telles que :
#   - Applications liées à des activités militaires, nucléaires, ou d'espionnage.
#   - Développement ou utilisation d'armes illégales, de drogues ou de substances contrôlées.
#   - Exploitation d'infrastructures critiques, de technologies de transport ou de machines lourdes.
#   - Promouvoir l'automutilation, le suicide ou toute autre forme de préjudice corporel.
#
#3. Tromperait ou induirait intentionnellement en erreur les autres, y compris :
#   - Générer ou promouvoir du contenu frauduleux, de la désinformation, ou des déclarations diffamatoires.
#   - Distribuer du spam ou se faire passer pour autrui sans consentement.
#   - Représenter à tort que les sorties du modèle sont générées par des humains.
#   - Faciliter un engagement en ligne faux, comme de fausses critiques.
#
#4. Ne divulgue pas de manière adéquate aux utilisateurs finaux les risques connus associés à votre système d'IA.
#
### 2. Signalement des Violations
#
#Les violations de cette Politique, ainsi que les bugs logiciels ou les problèmes de sécurité, doivent être signalés via les canaux suivants :
#- Signaler des problèmes avec le modèle : [https://github.com/ChickiLM](https://github.com/ChickiLM)
#- Signaler des bugs et des problèmes de sécurité : Chicki-Bugs@llm.ci
#- Signaler des violations de la Politique d'Utilisation Acceptable : Chicki-Viola@llm.ci
#
### 3. Avertissement
#
#LE MODÈLE EST FOURNI "TEL QUEL", SANS AUCUNE GARANTIE, EXPRESSE OU IMPLICITE, Y COMPRIS MAIS SANS S'Y LIMITER AUX GARANTIES DE QUALITÉ MARCHANDE, D'ADÉQUATION À UN USAGE PARTICULIER, ET DE NON-VIOLATION. EN AUCUN CAS LE DÉTENTEUR DU DROIT D'AUTEUR OU LES CONTRIBUTEURS NE SERONT RESPONSABLES DE TOUTE RÉCLAMATION, DOMMAGE OU AUTRE RESPONSABILITÉ, QUE CE SOIT DANS LE CADRE D'UN CONTRAT, D'UN DÉLIT OU AUTREMENT, DÉCOULANT DE, DEPUIS OU EN LIEN AVEC LE MODÈLE OU L'UTILISATION OU AUTRES INTERACTIONS AVEC LE MODÈLE.
#
### 4. Droit Applicable
#
#Cette licence est régie et interprétée conformément aux lois françaises, à l'exclusion de ses dispositions relatives aux conflits de lois.
#
### 5. Résiliation
#
#Cette licence est effective jusqu'à sa résiliation. Vos droits en vertu de cette licence seront automatiquement résiliés sans préavis si vous ne respectez pas les termes. À la résiliation, vous devez cesser toute utilisation et détruire toutes les copies du modèle.
#
### 6. Informations de Contact
#
#Pour toute question ou problème lié à cette licence, veuillez contacter Chicki à Chicki-Contact@llm.ci.
#
#
#
## Chicki License Agreement
#
#Copyright (c) 2024 Chicki. All rights reserved.
#
#This license governs the use of the Chicki model. By using, copying, or distributing this model, you agree to comply with the following terms and conditions.
#
### 1. Acceptable Use Policy
#
#You must comply with the Chicki Acceptable Use Policy as detailed below. This includes restrictions on illegal activities, harmful behavior, and misuse of the model.
#
#### 1.1 Prohibited Uses
#You agree not to use the model, nor allow others to use it, for any purpose that:
#
#1. Violates any law or infringes the rights of others, including:
#   - Engaging in or promoting illegal activities such as violence, terrorism, human trafficking, exploitation of minors, or any other unlawful act.
#   - Facilitating harassment, abuse, discrimination, or unauthorized practice of any profession.
#   - Collecting or processing sensitive personal data without proper authorization.
#   - Creating or distributing malicious software or otherwise disrupting digital systems.
#
#2. Involves activities that pose a risk of physical harm or death to individuals, such as:
#   - Applications related to military, nuclear, or espionage activities.
#   - Development or use of illegal weapons, drugs, or controlled substances.
#   - Exploitation of critical infrastructure, transportation technologies, or heavy machinery.
#   - Promoting self-harm, suicide, or other forms of bodily harm.
#
#3. Intentionally deceives or misleads others, including:
#   - Generating or promoting fraudulent content, misinformation, or defamatory statements.
#   - Distributing spam or impersonating others without consent.
#   - Misrepresenting the model's outputs as being generated by humans.
#   - Facilitating false online engagement, such as fake reviews.
#
#4. Fails to adequately inform end-users about known risks associated with your AI system.
#
### 2. Reporting Violations
#
#Violations of this Policy, as well as software bugs or security issues, should be reported through the following channels:
#- Report issues with the model: [https://github.com/ChickiLM](https://github.com/ChickiLM)
#- Report bugs and security issues: Chicki-Bugs@llm.ci
#- Report Acceptable Use Policy violations: Chicki-Viola@llm.ci
#
### 3. Disclaimer
#
#THE MODEL IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT, OR OTHERWISE, ARISING FROM, OUT OF, OR IN CONNECTION WITH THE MODEL OR THE USE OR OTHER DEALINGS IN THE MODEL.
#
### 4. Governing Law
#
#This license shall be governed by and construed in accordance with the laws of French, excluding its conflict of law provisions.
#
### 5. Termination
#
#This license is effective until terminated. Your rights under this license will terminate automatically without notice if you fail to comply with the terms. Upon termination, you must cease all use and destroy all copies of the model.
#
### 6. Contact Information
#
#For any inquiries or issues related to this license, please contact Chicki at Chicki-Contact@llm.ci
"""Classes de tokenisation pour Chicki."""
"""Tokenization classes for Chicki."""

from typing import Optional, Tuple

from transformers.tokenization_utils import AddedToken
from transformers.tokenization_utils_fast import PreTrainedTokenizerFast
from transformers.utils import logging
from tokenization_chicki import ChickiTokenizer

logger = logging.get_logger(__name__)

VOCAB_FILES_NAMES = {
    "vocab_file": "vocab.json",
    "merges_file": "merges.txt",
    "tokenizer_file": "tokenizer.json",
}

MAX_MODEL_INPUT_SIZES = {"ChickiLM/Chicki-tokenizer": 32768}

class ChickiTokenizerFast(PreTrainedTokenizerFast):
    """
    Construisez un tokenizer Chicki "fast" (soutenu par la bibliothèque *tokenizers* de HuggingFace). Basé sur le codage Byte-Pair-Encoding au niveau des octets.

    De même que GPT2Tokenizer, ce tokenizer a été formé pour traiter les espaces comme des parties des tokens, donc un mot sera
    encodé différemment qu'il soit au début de la phrase (sans espace) ou non:

    ```python
    >>> from transformers import ChickiTokenizerFast

    >>> tokenizer = ChickiTokenizerFast.from_pretrained("ChickiLM/Chicki-tokenizer")
    >>> tokenizer("Hello world")["input_ids"]
    [9707, 1879]

    >>> tokenizer(" Hello world")["input_ids"]
    [21927, 1879]
    ```
    Ceci est attendu.

    Ce tokenizer hérite de [`PreTrainedTokenizerFast`] qui contient la plupart des méthodes principales. Les utilisateurs doivent
    se référer à cette superclasse pour plus d'informations sur ces méthodes.

    Args:
        vocab_file (`str`, *optional*):
            Chemin vers le fichier de vocabulaire.
        merges_file (`str`, *optional*):
            Chemin vers le fichier de fusion.
        tokenizer_file (`str`, *optional*):
            Chemin vers le fichier [tokenizers](https://github.com/huggingface/tokenizers) (a généralement une extension .json) qui
            contient tout ce qui est nécessaire pour charger le tokenizer.
        unk_token (`str`, *optional*, par défaut `"<|endoftext|>"`):
            Le jeton inconnu. Un jeton qui n'est pas dans le vocabulaire ne peut pas être converti en ID et est défini comme ce
            jeton à la place. Ne s'applique pas à ce tokenizer.
        bos_token (`str`, *optional*):
            Le jeton de début de séquence. Non applicable pour ce tokenizer.
        eos_token (`str`, *optional*, defaults to `"<|endoftext|>"`):
            Le jeton de fin de séquence.
        pad_token (`str`, *optional*, defaults to `"<|endoftext|>"`):
            Le jeton utilisé pour le remplissage, par exemple lors du regroupement de séquences de différentes longueurs.

    Construct a "fast" Chicki tokenizer (backed by HuggingFace's *tokenizers* library). Based on byte-level
    Byte-Pair-Encoding.

    Same with GPT2Tokenizer, this tokenizer has been trained to treat spaces like parts of the tokens so a word will
    be encoded differently whether it is at the beginning of the sentence (without space) or not:

    ```python
    >>> from transformers import ChickiTokenizerFast

    >>> tokenizer = ChickiTokenizerFast.from_pretrained("ChickiLM/Chicki-tokenizer")
    >>> tokenizer("Hello world")["input_ids"]
    [9707, 1879]

    >>> tokenizer(" Hello world")["input_ids"]
    [21927, 1879]
    ```
    This is expected.

    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should
    refer to this superclass for more information regarding those methods.

    Args:
        vocab_file (`str`, *optional*):
            Path to the vocabulary file.
        merges_file (`str`, *optional*):
            Path to the merges file.
        tokenizer_file (`str`, *optional*):
            Path to [tokenizers](https://github.com/huggingface/tokenizers) file (generally has a .json extension) that
            contains everything needed to load the tokenizer.
        unk_token (`str`, *optional*, defaults to `"<|endoftext|>"`):
            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
            token instead. Not applicable to this tokenizer.
        bos_token (`str`, *optional*):
            The beginning of sequence token. Not applicable for this tokenizer.
        eos_token (`str`, *optional*, defaults to `"<|endoftext|>"`):
            The end of sequence token.
        pad_token (`str`, *optional*, defaults to `"<|endoftext|>"`):
            The token used for padding, for example when batching sequences of different lengths.
    """

    vocab_files_names = VOCAB_FILES_NAMES
    model_input_names = ["input_ids", "attention_mask"]
    slow_tokenizer_class = ChickiTokenizer

    def __init__(
        self,
        vocab_file=None,
        merges_file=None,
        tokenizer_file=None,
        unk_token="<|endoftext|>",
        bos_token=None,
        eos_token="<|endoftext|>",
        pad_token="<|endoftext|>",
        **kwargs,
    ):
        # Nous devons au moins passer vocab_file et merges_file à la classe de base
        # au cas où un tokenizer lent doit être initialisé ; d'autres peuvent être
        # configurés via des fichiers.
        # après GPT2TokenizerFast, en ajoutant également unk_token, bos_token et eos_token
        
        # We need to at least pass vocab_file and merges_file to base class
        # in case a slow tokenizer needs to be initialized; other can be
        # configured through files.
        # following GPT2TokenizerFast, also adding unk_token, bos_token, and eos_token

        unk_token = (
            AddedToken(unk_token, lstrip=False, rstrip=False, special=True, normalized=False)
            if isinstance(unk_token, str)
            else unk_token
        )
        bos_token = (
            AddedToken(bos_token, lstrip=False, rstrip=False, special=True, normalized=False)
            if isinstance(bos_token, str)
            else bos_token
        )
        eos_token = (
            AddedToken(eos_token, lstrip=False, rstrip=False, special=True, normalized=False)
            if isinstance(eos_token, str)
            else eos_token
        )
        pad_token = (
            AddedToken(pad_token, lstrip=False, rstrip=False, special=True, normalized=False)
            if isinstance(pad_token, str)
            else pad_token
        )

        super().__init__(
            vocab_file=vocab_file,
            merges_file=merges_file,
            tokenizer_file=tokenizer_file,
            unk_token=unk_token,
            bos_token=bos_token,
            eos_token=eos_token,
            pad_token=pad_token,
            **kwargs,
        )

    # Copié de transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast.save_vocabulary
    # Copied from transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast.save_vocabulary
    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:
        files = self._tokenizer.model.save(save_directory, name=filename_prefix)
        return tuple(files)

